<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> vcmi-gym | Simeon Manolov </title> <meta name="author" content="Simeon Manolov"> <meta name="description" content="An AI for the game of " heroes of might and magic iii> <meta name="keywords" content="ai-projects, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%95%B9%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smanolloff.github.io/projects/vcmi-gym/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?eb1d88d288af1dac35de430dae5a0768"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Simeon¬†</span> Manolov </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">vcmi-gym</h1> <p class="post-description">An AI for the game of "Heroes of Might and Magic III"</p> </header> <article> <p>You must have heard about <a href="https://en.wikipedia.org/wiki/Heroes_of_Might_and_Magic_III" rel="external nofollow noopener" target="_blank">Heroes of Might and Magic III</a> - a game about <strong>strategy</strong> and <strong>tactics</strong> where you gather resources, build cities, manage armies, explore the map and fight battles in an effort to defeat your opponents. The game is simply awesome and has earned a very special place in my heart.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-adventure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-battle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-town.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ah, just looking at these images makes me want to <i>install the game right now!</i><br>If you know, you know :) </div> <p>Released back in 1999, it‚Äôs still praised by a huge community of fans that keeps creating new game content, expansion sets, gameplay mods, HD remakes and whatnot. That‚Äôs where <a href="https://vcmi.eu/" rel="external nofollow noopener" target="_blank">VCMI</a> comes in: a fan-made <strong>open-source recreation</strong> of HOMM3‚Äôs engine ‚ù§Ô∏è As soon as I heard about it, my eyes sparkled and I knew what my next project was going to be: an AI for HOMM3.</p> <h3 id="problem-statement">Problem Statement</h3> <p>The game features scripted AI opponents which are not good at playing the game and are no match for an experienced human player. To compensate for that, the AI is <em>cheating</em> - i.e. starts with more resources, higher-quality fighting units, fully revealed adventure map, etc. Pretty lame.</p> <h3 id="objective">Objective</h3> <p>Create a non-cheating AI that is better at playing the game.</p> <h3 id="proposed-solution">Proposed Solution</h3> <p>Transform VCMI into a reinforcement learning environment and forge an AI which meets the objective.</p> <p>Contribute to the VCMI project by submitting the pre-trained AI model along with a minimal set of code changes needed for adding an option to enable it via the UI.</p> <h3 id="approach">Approach</h3> <p>Given that the game consists of several distinct player perspectives (combat, adventure map, town and hero management), training separate AI models for each of them, starting with the simplest one, seems like a good approach.</p> <h5 id="phase-1-preparation">Phase 1: Preparation</h5> <ol> <li>Explore VCMI‚Äôs codebase <ul> <li>gather available documentation</li> <li>read, set-up project locally &amp; debug</li> <li>if needed, reach out to VCMI‚Äôs maintainers</li> <li>document as much as possible along the way</li> </ul> </li> <li>Research how to interact with VCMI (C++) from the RL env (Python) <ul> <li>best-case scenario: load C++ libraries into Python and call C++ functions directly</li> <li>fallback scenario: launch C++ binaries as Python sub-processes and use inter-process communication mechanisms to simulate function calls</li> </ul> </li> </ol> <h5 id="phase-2-battle-only-ai">Phase 2: Battle-only AI</h5> <ol> <li>Create a VCMI battle-only RL environment <ul> <li>follow the Farama Gymnasium (ex. OpenAI Gym) API standard</li> <li>optimise VCMI w.r.t. performance (e.g. no UI, fast restarts, etc.)</li> </ul> </li> <li>Train a battle-only AI model <ul> <li>observation design: find the optimal amount of data to extract for at each timestep</li> <li>reward design: find the optimal reward (or punishment) at each timestep</li> <li>train/test data: generate a large and diverse data set (maps, army compositions) to prevent overfitting.</li> <li>start by training against the VCMI scripted AI, then gradually introduce self-training by training vs older versions of the model itself. Optionally, develop a MARL (multi-agent RL) where two two agents are trained concurrently.</li> <li>observability: use W&amp;B and Tensorboard to monitor the training performance</li> </ul> </li> <li>Bundle the pre-trained model into a VCMI mod called ‚ÄúMMAI‚Äù which replaces VCMI‚Äôs default battle AI</li> <li>Contribute to the VCMI project by submitting the MMAI plugin along with the minimal set of changes to the VCMI core needed by the plugin</li> <li>Contribute to the Gymnasium project by submitting vcmi-gym as an official third-party RL environment</li> </ol> <h5 id="phase-3-adventure-only-ai">Phase 3: Adventure-only AI</h5> <p>Worthy of a separate project on its own, training an Adventure AI out of scope for now. A detailed action plan is not yet required.</p> <h2 id="dev-log">Dev log</h2> <p>I will be outlining parts of the project‚Äôs development lifecycle, focusing on those that seemed most impactful.</p> <p>Since I started this project back in 2023, there will be features in VCMI (and vcmi-gym) that were either introduced, changed or removed since then, as both are actively evolving. Still, most of what is written here should be pretty much accurate for at least a few years time.</p> <h4 id="setting-up-vcmi">Setting up VCMI</h4> <p>First things first - I needed to start VCMI locally in debug mode. Thankfully, the VCMI devs have provided a nice <a href="https://github.com/vcmi/vcmi/blob/develop/docs/developers/Building_macOS.md" rel="external nofollow noopener" target="_blank">guide</a> for that. Some extra steps were needed in my case (most notably due to Qt installation errors), for which I decide to prepare a step-by-step vcmi-gym <a href="https://github.com/smanolloff/vcmi-gym?tab=readme-ov-file#installation" rel="external nofollow noopener" target="_blank">setup guide</a> in the project‚Äôs official git page. the installation notes below:</p> <p>Except for a few hiccups, the process of setting up VCMI went smooth. It was educational and gave me some basic, but useful knowledge:</p> <ul> <li>CMake is a tool for compiling many C++ source files with a single cmake command. A must for any project consisting of more than a few source files.</li> <li> <code class="language-plaintext highlighter-rouge">conan</code> is like <code class="language-plaintext highlighter-rouge">pip</code> for C++</li> <li>Debugging C++ code can be done in many different ways, so I made sure to experiment with a few of them: <ul> <li> <code class="language-plaintext highlighter-rouge">lldb</code> can be used directly as a command-line debugging tool. I feel quite comfortable with CLIs and I looked forward to using this one, but the learning curve was a bit steep for me, so opted for a GUI this time.</li> <li>Sublime Text‚Äôs C++ debugger was a disappointment. I am a <strong>huge</strong> fan of ST and consider it the best text editor out there, but it‚Äôs simply not good for debugging.</li> <li>VSCode‚Äôs debugger really saved the day. Has some glitches, but is really easy to work with. I will always prefer ST over VSC, but I do have it installed at all times, just for its debugger capabilities.</li> <li>Xcode did not work well. VCMI is not a project that does not follow many Xcode conventions (naming, file/directory structure, etc.) which made it hard to simply navigate through the codebase. Well, IDEs have never felt comfortable anyway, so there was no need to waste my time further here.</li> </ul> </li> <li>C++ language feature support (completions, linting, navigation, etc.) in Sublime Text 4 is provided by the <a href="https://github.com/sublimelsp/LSP-clangd" rel="external nofollow noopener" target="_blank">LSP-clangd</a> package and is <em>awesome</em>. I found it vastly superior to VSCode‚Äôs buggy C++ extension.</li> </ul> <p>Unfortunately, VCMI‚Äôs dev setup guide was like a nice welcome-drink on a party where nothing else is included. That‚Äôs to say, there was no useful documentation for me beyond that point. No surprises here - I‚Äôve worked on enough projects where keeping it all documented is a <em>hopeless</em> endeavor, so I don‚Äôt blame anyone. My hope is that the code is well-structured and easy to understand. Good luck with that - the codebase is over 300K lines of C++ code.</p> <h4 id="so-how-does-it-work">So‚Ä¶ how does it work?</h4> <p>Having it up &amp; running, it was time to delve into the nitty-gritty of the VCMI internals and start connecting the dots.</p> <h5 id="vcmis-client-server-communication-protocol">VCMI‚Äôs client-server communication protocol</h5> <p>The big picture is a classic client-server model with a single server (owner of the global game state) and many clients (user interfaces).</p> <p>Communication is achieved via TCP where the application data packets are serialized versions of <code class="language-plaintext highlighter-rouge">CPack</code> objects, for example:</p> <div class="row"> <div class="col-sm-3 offset-sm-2 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-newgame-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-makeaction-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><i>Note: the term ‚ÄúLobby‚Äù in the diagrams refers to the game‚Äôs Main Menu (in this case - ‚ÄúNew Game‚Äù screen) and should not be confused with VCMI‚Äôs lobby component for online multiplayer.</i></p> <p><code class="language-plaintext highlighter-rouge">MakeAction</code>, <code class="language-plaintext highlighter-rouge">StartAction</code>, etc. are sub-classes of the <code class="language-plaintext highlighter-rouge">CPack</code> base class ‚Äì since C++ is a strongly typed language, there‚Äôs a separate class object for each different data structure:</p> <div class="row"> <div class="col-sm-4 offset-sm-4 mt-4 offset-mt-4 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-cpack-wbs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The <code>CPack</code> class tree (SVG version <a href="/assets/img/vcmi-gym/diagram-cpack-wbs.svg" target="_blank">here</a>) </div> <p>Whenever I have to deal with a proprietary network communication protocol which I am not familiar with, I tend to examine the raw data sent over the wire to make sure I am not missing anything. So I sniffed a couple of TCP packets using Wireshark and here‚Äôs what the raw data looks like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== MakeAction command packet. Format:
==
== [32-bit hex dump] // [value in debugger] // [dtype] // [field desc]

01           // \x01  ui8   hlp (true, i.e. not null)
FA 00        // 250   ui16  tid (typeid)
00           // \0    ui8   playerColor
29 00 00 00  // 41    si32  requestID
00           // 0     ui8   side
00 00 00 00  // 0     ui32  stackNumber
02 00 00 00  // 2     si32  actionType
FF FF FF FF  // -1    si32  actionSubtype
01 00 00 00  // 1     si32  length of (unitValue, hexValue) tuples
18 FC FF FF  // -1000 si32  unitValue (INVALID_UNIT_ID)
45 00        // 69    si16  hexValue.hex
</code></pre></div></div> <p>The observed hex dump aligns with (parts) of the object information displayed in the debugger, as well the class declarations for <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/NetPacks.h#L2500" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MakeAction</code></a>, its member variable of type <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/battle/BattleAction.h#L24" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">BattleAction</code></a> field and its parent <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/NetPacksBase.h#L95" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CPackForServer</code></a>. The communication protocol between the server and the clients is now clear.</p> <p>However, the data itself does not tell much about the system‚Äôs behaviour. There‚Äôs a lot going on after that data is received ‚Äì let‚Äôs see what.</p> <h5 id="data-processing">Data processing</h5> <p>Any packet that is accepted by a client or server essentially spins a bunch of gears which ultimately change the global game state and, optionally, result in other data packets being sent in response.</p> <p>Figuring out the details by navigating through the VCMI codebase is nearly impossible with the naked eye ‚Äì there are ~300K lines of code in there. That‚Äôs where the debugger really comes in handy ‚Äì with it, I followed the code path of the received data from start to end and mapped out the important processing components involved. Digging deeper into the example above, I visualized the handling of a <code class="language-plaintext highlighter-rouge">LobbyClientConnected</code> packet from two different perspectives in an attempt to get a grip on what‚Äôs going on:</p> <div class="row"> <div class="col-sm-2 offset-sm-1 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-lobbyclientconnected-activity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-lobbyclientconnected-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <code>LobbyClientConnected</code> server packet handling (SVG versions <a href="/assets/img/vcmi-gym/diagram-lobbyclientconnected-activity.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-lobbyclientconnected-routing.svg" target="_blank">here</a>) </div> <p>That‚Äôs the ‚Äúshort‚Äù version, anyway ‚Äì many irrelevant details and function calls are omitted from the diagrams to keep it readable. But it‚Äôs just one of many possible codepaths, considering the number of different packets (see the <code class="language-plaintext highlighter-rouge">CPack</code> class tree above). It would be impossible to map them all, but I did map few more as I kept exploring the codebase ‚Äì I am sharing them here as they might come in handy in the future:</p> <div class="row"> <div class="col-sm-4 offset-sm-1 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-heromoved-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-2 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-playerblocked-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-makeaction-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> More packet routing diagrams (SVG versions: <a href="/assets/img/vcmi-gym/diagram-heromoved-routing.svg" target="_blank">here</a>, <a href="/assets/img/vcmi-gym/diagram-playerblocked-routing.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-makeaction-routing.svg" target="_blank">here</a>) </div> <p>Patterns of the processing logic began to emerge at that point, meaning I had reached a satisfactory level of general understanding about how VCMI works. It was time to think about transforming it into a reinforcement learning environment.</p> <h4 id="optimizing-vcmi-for-rl">Optimizing VCMI for RL</h4> <p>VCMI‚Äôs user-oriented design makes it unsuitable for training AI models efficiently. On-policy RL algorithms like PPO are designed to operate in environments where state observations can collected at high speeds and training a battle AI is a process that will involve a <em>lot</em> of battles being played. We are talking millions here.</p> <h5 id="quick-battles">Quick battles</h5> <p>With the animation speed set to max and auto-combat enabled, a typical 10 round battle takes around a minute. <a href="https://www.youtube.com/watch?v=hV_2Q-sjYOA" rel="external nofollow noopener" target="_blank"><em>Ain‚Äôt nobody got time for that</em></a>.</p> <p>The game features a ‚Äúquick combat‚Äù setting which causes battles to be carried out in the background without any user interaction (the user‚Äôs troops are controlled by the computer instead). With quick combat enabled, a battle gets resolved in under a second, which is a great improvement. Clearly, combat <strong>training should be conducted in the form of quick combats</strong>.</p> <p>But what good is a sub-second combat if it takes 10+ seconds to restart?</p> <h5 id="quick-restarts">Quick restarts</h5> <p>Fortunately, VCMI features the potentially helpful ‚Äúquick combat replays‚Äù setting. Sadly, it allows only a <em>single manual replay</em> per battle and only when the enemy is a neutral army - not particularly useful in my case. What I need is <em>infinite quick replays</em>.</p> <p>A deeper look into VCMI‚Äôs internals reveals the query stack, where each item roughly corresponds to an event whose outcome depends on other events which might occur in the meantime. When the player chooses to restart combat, the results from battle query #2 are not applied and the entire query is re-inserted back in the stack (with a few special flags set).</p> <div class="row"> <div class="col-sm-7 offset-sm-1 mt-7 offset-mt-1 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/querystack.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-replaybattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The VCMI query stack and a communication diagram for (re-)starting combat (SVG <a href="/assets/img/vcmi-gym/diagram-replaybattle-sequence.svg" target="_blank">here</a>) </div> <p>Removing the restart battle restrictions involved relatively minor code changes and even revealed a small <a href="https://github.com/vcmi/vcmi/issues/953#issuecomment-1787151606" rel="external nofollow noopener" target="_blank">memory leak</a> in VCMI itself (at least for VCMI v1.3.2, it should already be fixed in v1.4+)</p> <h5 id="benchmarks">Benchmarks</h5> <p>My poor man‚Äôs benchmark setup consists of a simple 2-player micro adventure map (2x2) where two opposing armies of similar strength (heroes with 7 groups of units each) engage in a battle which is restarted immediately after it ends:</p> <div class="row justify-content-md-center"> <div class="col-sm-8 mt-8 offset-mt-2 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/testmap-layout.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A very simple map for testing purposes </div> <p>The benchmark was done on my own laptop (M2 Macbook Pro). Here are the measurements:</p> <ul> <li>16.28 battles per per second</li> <li>862 actions per second (431 per side)</li> </ul> <p>The results were good enough for me - there was no need to optimize VCMI further at this point. It was about time I started thinking on how to integrate it with the Python environment.</p> <h4 id="embedding-vcmi">Embedding VCMI</h4> <p>The <a href="https://gymnasium.farama.org/" rel="external nofollow noopener" target="_blank">Farama Gymnasium</a> (formerly OpenAI Gym) API standard is a Python library that aims to make representing RL problems easier. I like it because of its simplicity and wide adoption rate within the python RL community (RLlib, StableBaselines, CleanRL are a few examples), so I going to use it for my RL environment.</p> <p>Communicating with a C++ program (i.e. VCMI) from a Python program was a new and exciting challenge for me. Given that the Python interpreter itself is written in C++, it had to be possible. I googled a bit and decided to go with <a href="https://pybind11.readthedocs.io/en/stable/" rel="external nofollow noopener" target="_blank">pybind11</a>, which looked like the tool for the job.</p> <p>I quickly ran into various issues related to memory violations. Anyone that has worked with data pointers (inevitable in C++) has certainly encountered the infamous <em>Undefined Behaviour‚Ñ¢</em> monster that can make a serious mess out of any program. Turns out there is a very strict line one must not cross when embedding C++ in Python: accessing data outside of a python thread without having acquired the Global Interpreter Lock (GIL). A Python developer never really needs to think about it until they step out of Python Wonderland and enter the dark dungeons of a C++ extension. I managed to eventually get the hang of it and successfully integrated both programs. A truly educational experience.</p> <p>While experimenting with pybind11, I was surprised to find out VCMI can‚Äôt really be <em>embedded</em> as it refuses to boot in a non-main thread. Definitely a blocker, since the main thread during training is the RL script, not VCMI. Bummer.</p> <p>A quick investigation revealed that the <a href="https://www.libsdl.org/" rel="external nofollow noopener" target="_blank">SDL</a> loop which renders the graphical user interface (GUI) was responsible for the issue. This GUI had to go.</p> <h5 id="removing-the-gui">Removing the GUI</h5> <p>There have always been many reasons to remove the GUI ‚Äì it is not used during training, consumes additional hardware resources and enforces a limit on the overall game speed due to hard-coded framerate restrictions, so I was more than happy to deal with it now that it became necessary.</p> <p>The VCMI executable accepts a <code class="language-plaintext highlighter-rouge">--headless</code> flag which causes a runtime error as soon as the game is started. Still, the codebase did contain code paths for running in such a mode, so making it work properly should be an easy win. In the end, I decided to introduce a new build target which defines a function which only <em>initializes</em> the SDL (this is required for the game to run) and another function which starts the game but <em>without</em> activating the SDL render loop.</p> <p>However, with no GUI, it was hard to see what‚Äôs going on, so I ended up coding a text-based renderer which is pretty useful for visualizing battlefield state in the terminal:</p> <div class="row"> <div class="col"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-ansi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> My ANSI text renderer for VCMI \o/ </div> <p>After pleasing the eye with such a result, it was time to get back to the VCMI-Python integration with pybind11.</p> <h5 id="connecting-the-pieces">Connecting the pieces</h5> <p>I refrained from the quick-and-dirty approach even for PoC purposes as it meant polluting with pybind11 code and dependencies all over the place. A separate component had to be designed for the purpose.</p> <p>Typically, connecting two components with incompatible interfaces involves an adapter (I call it <em>connector</em>) which provides a clean API to each of the components:</p> <div class="row justify-content-md-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povgym-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povconnector-components.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povvcmi-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>When applying the <a href="https://refactoring.guru/design-patterns/adapter" rel="external nofollow noopener" target="_blank">adapter pattern</a> one must flip their perspective from the local viewpoint of an object in a relationship, to the shared viewpoint of the relationship itself (i.e. both sides of our connector). That‚Äôs when one issue becomes apparent: both components are controlled by <em>different</em> entities - i.e. the VCMI client receives input from the VCMI Server, while the gym env receives input from the RL algorithm.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povconnector-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A view on the vcmi-gym relationships through the connector </div> <p>Since both controlling entities (RL script and VCMI server) are otherwise unrelated, the <em>connector</em> is responsible for ensuring that they operate in a mutually synchronous manner. The solution involves usage of synchronization primitives to block the execution of one thread while the other is unblocked:</p> <div class="row justify-content-md-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-init-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-step-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-reset-endbattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-reset-midbattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Connector implementation details (SVG versions <a href="/assets/img/vcmi-gym/diagram-connector-init-sequence.svg" target="_blank">here</a>, <a href="/assets/img/vcmi-gym/diagram-connector-step-sequence.svg" target="_blank">here</a> <a href="/assets/img/vcmi-gym/diagram-connector-reset-endbattle-sequence.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-connector-reset-midbattle-sequence.svg" target="_blank">here</a>) </div> <p>Some notes regarding the diagrams above:</p> <ul> <li>The gray background denotes a group of actors operating within the same thread (where <code class="language-plaintext highlighter-rouge">T1</code>, <code class="language-plaintext highlighter-rouge">T2</code>, ‚Ä¶ are the threads). The same actor can operate in multiple threads.</li> <li>The meaning behind the color-coded labels is as follows: <ul> <li> <span style="background-color: yellow; color: black">acquire lock</span>: a successful attempt to acquire the shared lock. The can be released explicitly via <span style="background-color: black; color: yellow">release lock</span> or implicitly at the end of the current call block.</li> <li> <span style="background-color: yellow; color: red">acquire lock</span>: an unsuccessful attempt to acquire the shared lock, effectively blocking the current thread execution until the lock is released.</li> <li> <span style="background-color: red; color: black">cond.wait</span>: the current thread execution is blocked until another thread notifies it via <span style="color: blue">cond.notify</span> (<code class="language-plaintext highlighter-rouge">cond</code> is a <a href="https://en.cppreference.com/w/cpp/thread/condition_variable" rel="external nofollow noopener" target="_blank">conditional variable</a>).</li> <li> <span style="color: gray">P_Result</span> and <span style="color: gray">Action</span>: shared variables modified by reference (affecting both threads)</li> </ul> </li> <li>The red bars indicate that the thread execution is blocked.</li> <li> <code class="language-plaintext highlighter-rouge">AAI</code> and <code class="language-plaintext highlighter-rouge">BAI</code> are the names of my C++ classes which implement VCMI‚Äôs <code class="language-plaintext highlighter-rouge">CAdventureAI</code> and <code class="language-plaintext highlighter-rouge">CBattleGameInterface</code> interfaces. Each VCMI AI (even the default scripted one) defines such classes.</li> <li> <code class="language-plaintext highlighter-rouge">baggage</code> is a special struct which contains a reference to the <code class="language-plaintext highlighter-rouge">GetAction</code> function. I introduced the Baggage idiom in VCMI to enable <a href="https://en.wikipedia.org/wiki/Dependency_injection" rel="external nofollow noopener" target="_blank">dependency injection</a> - specifically, the Baggage struct is seen as a simple <code class="language-plaintext highlighter-rouge">std::any</code> object by upstream code, all the way up until <code class="language-plaintext highlighter-rouge">AAI</code> where it‚Äôs converted back to a Baggage struct and the function references stored within are extracted. A lot of it feels like <em>black magic</em> and implementing it was a real challenge (function pointers in C++ are really weird).</li> <li>at the end of a battle, a special flag is returned with the result which indicates that the next action must be <code class="language-plaintext highlighter-rouge">reset</code> </li> <li>in the middle of a battle, a <code class="language-plaintext highlighter-rouge">reset</code> is still a valid action which is effectively translated to <em>retreat</em> + an a ‚Äúyes‚Äù answer to the ‚ÄúRestart battle‚Äù dialog</li> </ul> <p>With that, the important parts of the VCMI-gym integration were now in place. There were many other changes which I won‚Äôt discuss here, such as dynamic TCP port allocation, logging improvements, fixes for race conditions when starting multiple VCMIs, building and loading it as a single shared dynamic library, etc. My hands already itched to work on the actual RL part which was just over the corner.</p> <h3 id="renforcement-learning">Renforcement Learning</h3> <p>This is the part I found the most difficult. While there are well-known popular RL algos out there, it‚Äôs not a simple matter of <em>plug&amp;play</em> (or <em>plug&amp;train</em> :)) It‚Äôs a lengthy process involving a cycle of of research, imeplentation, deployment, observation and analysis. It‚Äôs why I am always taken aback by my friends‚Äô reactions when telling them about this project:</p> <p>‚ÄúThe AI algorithms have already been developed, what takes you so long?‚Äù</p> <p>There‚Äôs so much I have to say here, but most often, I prefer to avoid talking for another hour about it, so I usually pick an answer as short as ‚ÄúTrust me, it‚Äôs not‚Äù. And it‚Äôs not a good answer, I know that. Call it bad marketing - not just my answer, about this project, but the ubiquitous hype around AI that has made people thinking <i>‚ÄúThat‚Äôs it, humanity has found the <em>formula for AI</em>. Now it‚Äôs all about having a good hardware‚Äù</i>. Well, if that were the case, I wouldn‚Äôt bother with this article in the first place. If you are interested in the long answer, read ahead.</p> <h4 id="the-rl-cycle">The RL cycle</h4> <p>There‚Äôs plenty of information about RL on the web, but I will outline the most essential part it: the action-steate-reward cycle which nicely fits into this simple diagram:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-diagram.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So far I had prepared VCMI as an RL environment, but that‚Äôs just one key piece. Next up is presenting its state to the agent.</p> <h5 id="Ô∏è-observations">üëÅÔ∏è Observations</h5> <p>Figuring out which parts of the environment state should be ‚Äúobservable‚Äù by the agent (i.e. the observation space) is a balance between providing enough, yet not too much information.</p> <p>I prefer the <a href="https://en.wikipedia.org/wiki/Empathic_design" rel="external nofollow noopener" target="_blank">Empathic design</a> approch when dealing with such a problem, trying to imagine myself playing the game (and, if possible, actually play it) with very limited information about its current state, taking notes of what‚Äôs missing and how important is it. For example, playing a game of Chess without seeing the pieces that have been taken out, or without seeing the enemy‚Äôs remaining time is OK, but things get rough if I can‚Äôt distinguish the different types of pieces, for example.</p> <p>Since the agent‚Äôs observation space is just a bunch of numbers organized in vectors, it would be nearly impossible for me to interpret it directly ‚Äì rather, I apply certain post-processing to the observation and transform it into something that my brain can use (yes, that is a sloppy way to describe ‚Äúuser interface‚Äù). The important part is this: all information I get to see is simply a projection of the information the agent gets to see.</p> <p>The observation space went through several design iterations, but I will only focus on the latest one.</p> <p>The mandatory component of the observation, the ‚Äúchess board‚Äù equivalent here is the battlefield‚Äôs terrain layout. I started out by enumerating all relevant battle hexes that would represent the basis of the observation. The enumeration looks similar to the one used by VCMI‚Äôs code, but is different.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/hexes.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Each hex can be in exactly one of four states w.r.t. the currently active unit:</p> <ol> <li>terrain obstacle</li> <li>occupied by a unit</li> <li>free (unreachable)</li> <li>free (reachable)</li> </ol> <p>This state is represented by a single number. In addition, there are 15 more numbers which represent the occupying creature‚Äôs attirbutes, similar to what the player would see when right-clicking on the creature: owner, quantity, creature type, attack, defence, etc.</p> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-lizardmen.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5"> <figure> <picture> <img src="/assets/img/vcmi-gym/table-observationspace.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>No information about the terrain‚Äôs type (grass, snow, etc.) as well as the creature‚Äôs morale, luck, abilities and status effects is provided to the agent. A cardinal sin of RL environment design is to ignore the <a href="https://en.wikipedia.org/wiki/Markov_property" rel="external nofollow noopener" target="_blank">Markov property</a>, whereby RL algorithms would struggle to optimize the policies, so it was important to not simply hide those attributes, but take them out of the equation:</p> <ul> <li>terrain is always ‚Äúcursed ground‚Äù (negates morale and luck)</li> <li>heroes have no spell books as well as no spellcasting units in their army</li> <li>heroes have no passive skills or artifacts affecting the units‚Äô stats</li> </ul> <p>The observation space could be expanded and the restrictions - removed as soon as the agent learns to play well enough.</p> <p><i>Note: this is already the case, and additional info was eventually added to the observation: morale, luck, a subset of the creature abilities, total army strength, etc.</i></p> <h5 id="Ô∏è-actions">üïπÔ∏è Actions</h5> <p>Designing the action space is definitely simpler as the agent should be able to perform any action as long as it‚Äôs possible under certain conditions. The only meaningful restrictions here are those that prevent quitting, retreating and (as discussed earlier) spell casting.</p> <p>The total number of possible actions then becomes <strong>2312</strong>, which is a sum of:</p> <ul> <li>1 ‚ÄúDefend‚Äù action</li> <li>1 ‚ÄúWait‚Äù action</li> <li>165 ‚ÄúMove‚Äù actions (1 per hex)</li> <li>165 ‚ÄúRanged attack‚Äù actions (1 per hex)</li> <li>165*12=1320 ‚ÄúMelee attack‚Äù actions (12 per hex - see image below)</li> </ul> <div class="row justify-content-md-center"> <div class="col-sm-3"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The 12 melee attacks per hex (6 to 11 are available to 2-hex attackers only) </div> <p>The fact is, most of those actions are only <em>conditionally available</em> at the current timestep. For example, the 1652 actions are reduced to around 600 if the active unit has a speed of <code class="language-plaintext highlighter-rouge">5</code>, as it simply can‚Äôt reach most of the hexes. If there are also no enemy units it can target, that number is further reduced to ~200 as the ‚Äúmelee attack‚Äù actions become unavailable. If it‚Äôs a melee unit, the ‚Äúranged attack‚Äù actions also become unavailable, reducing the possible actions to around 50 ‚Äì <em>orders of magnitute</em> lower.</p> <p>Having such a small fraction of valid actions could hinder the initial stage of the learning process, as the agent will be unlikely to select those actions. Introducing action masking can help with this problem ‚Äì more on that later.</p> <h5 id="-rewards">üç© Rewards</h5> <p>Arguably the hardest part is deciding when and how much to reward (or punish) agents for their actions. Sparse rewards, i.e. rewards only at the end of the episode, or battle, may lead to (much) slower learning, while too specific rewards (at every step, based on the particular action taken) may induce too much bias, ultimately preventing the agent from developing strategies which the reward designer did not account for.</p> <p>The reward can be expressed as:</p> \[R = \sum_{i=1}^nS_i(5D_{i} - V_i\Delta{Q_i})\] <p>where:</p> <ul> <li> <strong>R</strong> is the reward</li> <li> <strong>n</strong> is the number of stacks on the battlefield</li> <li> <strong>S<sub>i</sub></strong> is <code class="language-plaintext highlighter-rouge">1</code> if the stack is friendly, <code class="language-plaintext highlighter-rouge">-1</code> otherwise</li> <li> <strong>D<sub>i</sub></strong> is the damage dealt by the stack</li> <li> <strong>V<sub>i</sub></strong> is the stack‚Äôs <a href="https://heroes.thelazy.net/index.php/List_of_creatures" rel="external nofollow noopener" target="_blank">AI Value</a> </li> <li> <strong>ŒîQ<sub>i</sub></strong> is the change in the stack‚Äôs quantity (number of creatures)</li> </ul> <p>And a (simpler) alternative to the mathematical formulation above:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>R = 0
for stack in all_stacks:
    points = 5 * stack.dmg_dealt - (stack.ai_value * stack.qty_diff)
    sign = stack.is_friendly ? 1 : -1
    R += sign * points
</code></pre></div></div> <p>The bottom line is: all rewards are zero-sum, i.e. for each reward point given to one side, a corresponding punishment point is given to the other side.</p> <p>An important note here is that there‚Äôs no punishment for invalid actions (or <em>conditionally unavailable</em> actions) ‚Äì the initial reward design did include such, but it became redundant after I added action masking (see next section).</p> <p><i>Note: this reward function became more complex as the project evolved and eventually included damping factors, fixed per-step rewards, scaling based on total starting army value, etc. An essential part of it still corresponds to the above formula, though.</i></p> <h4 id="Ô∏è-training-algorithm">üèãÔ∏è Training algorithm</h4> <p>I started off with <a href="https://github.com/DLR-RM/stable-baselines3" rel="external nofollow noopener" target="_blank">stable-baselines3</a>‚Äôs PPO and DQN implementations, which I have used in the past, and launched a training session where the left-side army is controlled by the agent and the enemy (right-side army) is controlled by the built-in ‚ÄúStupidAI‚Äù bot:</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-battle-simple.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The idea was to eventually add more units to each army which would progressively increase the complexity.</p> <p>However, one issue quickly popped up: the action space was too big (2000+ actions), while the average number of <em>allowed</em> actions at any given timestep was less than 50. It means an untrained agent has only 2% chance to make an valid action (the initial policy is basically an RNG). The agent was making too many invalid actions and, although it eventually learned to stop making them, it still performed very poorly:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-issue-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The number of invalid actions ("errors") decreases, but the success rate (games won) stays at 0. </div> <p>Something was clearly wrong here. Why was the agent unable to win a game? Clearly it <em>was learning</em>: the <code class="language-plaintext highlighter-rouge">errors</code> chart shows it eventually stops making invalid actions. I needed more information.</p> <p>I started reporting aggregated metrics with the actions being taken and visualised them as a heatmap (had to write a custom <a href="https://vega.github.io/vega/" rel="external nofollow noopener" target="_blank">vega</a> chart in W&amp;B for that). The problem was revealed immediately: the agent‚Äôs policy was converging to a single action:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-issue-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Heatmaps of the agent's attempted actions throughout the training session. The agent learns to make only "Defend" actions. W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi/groups/M2-PPO-20231127_192222/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>). </div> <p>Apparently, all the agent ‚Äúlearned‚Äù was that any action except the ‚ÄúDefend‚Äù action results in a negative reward (recall that 98% of the actions are invalid at any given timestep). While it does yield a higher reward, it is still a poor strategy that will always lead to a loss. This is referred to as <em>local minima</em> and such a state is usually nearly impossible to recover from.</p> <p>One option was to try and tweak the negative feedback for invalid actions. Making it too small or completely removing it caused another local minima: convergence into a policy where only invalid actions were taken, as the episode never finished and thus - the enemy could never inflict any damage (the enemy is not given a turn until the agent finishes its own turn by making a valid move). Maybe I should introduce a negative feedback on ‚ÄúDefend‚Äù actions‚Ä¶? That would introduce too much bias (defending is not an inherently <em>bad</em> action, but the agent would perceive it as such).</p> <p>Reshaping the reward signal was not productive here. I had to prevent such actions from being taken in the first place. To do this, I needed to understand how agents choose their actions:</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-softmax.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>The output layer of a neural network produces a group of <code class="language-plaintext highlighter-rouge">n</code> values (called <em>logits</em>): <code class="language-plaintext highlighter-rouge">[s‚ÇÅ, s‚ÇÇ, ..., s‚Çô]</code>, where <code class="language-plaintext highlighter-rouge">n</code> is the number of actions. The logits have no predefined range, i.e. <code class="language-plaintext highlighter-rouge">s·µ¢ ‚àà (-‚àû, ‚àû)</code>.</li> <li>A <code class="language-plaintext highlighter-rouge">softmax</code> opration forms a discrete <em>probability distribution</em> <code class="language-plaintext highlighter-rouge">P</code>, i.e. the values are transformed into <code class="language-plaintext highlighter-rouge">[p‚ÇÅ, p‚ÇÇ, ..., p‚Çô]</code> where <code class="language-plaintext highlighter-rouge">p·µ¢ ‚àà [0, 1]</code> and <code class="language-plaintext highlighter-rouge">sum(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô) = 1</code>.</li> <li>Depending on the policy, either of the following comes next: <ul> <li>Greedy policy: an <code class="language-plaintext highlighter-rouge">argmax(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code> operation (denoted as <code class="language-plaintext highlighter-rouge">max(p·µ¢)</code> in the figure above) returns the <em>index</em> <code class="language-plaintext highlighter-rouge">i</code> of the value <code class="language-plaintext highlighter-rouge">p·µ¢ = max(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code>, i.e. the action with the highest probability.</li> <li>Stochastic policy: a <code class="language-plaintext highlighter-rouge">sample(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code> operation (not shown in the figure above) returns the <em>index</em> <code class="language-plaintext highlighter-rouge">i</code> of a sampled value <code class="language-plaintext highlighter-rouge">p·µ¢ ~ P</code>.</li> </ul> </li> </ol> <p>Armed with that knowledge, it‚Äôs clear that if the output logits are infinitely small, the probability of their corresponding actions will be <code class="language-plaintext highlighter-rouge">0</code>. This is called <strong>action masking</strong> and is exactly what I needed.</p> <p>Action masking in PPO was already supported in stable-baselines3‚Äôs <a href="https://github.com/Stable-Baselines-Team/stable-baselines3-contrib" rel="external nofollow noopener" target="_blank">contrib</a> package, but was not readily available for any other algorithm. If I wanted such a feature for, say, QRDQN, I was to implement it myself. This turned out to be quite difficult in stable-baselines3, so I decided to search for other RL algorithm implementations which would be more easily extensible. That‚Äôs how I found <a href="https://github.com/vwxyzjn/cleanrl" rel="external nofollow noopener" target="_blank">cleanrl</a>.</p> <p>CleanRL is a true gem when it comes to python implementations of RL algorithms: the the code is simple, concise, self-contained, easy to use and customize, excellent for both practical and educational purposes. It made it easy for me to add action masking to PPO and QRDQN (an RL algorithm which I implemented on top of cleanrl‚Äôs DQN). Prepending an ‚ÄúM‚Äù to those algos was my way of marking them as augmented with action masking (MPPO, MQRDQN, etc).</p> <p>A detailed analysis of the effects of action masking can be found in this <a href="https://arxiv.org/pdf/2006.14171" rel="external nofollow noopener" target="_blank">paper</a>. I would also recommend this <a href="https://boring-guy.sh/posts/masking-rl/" rel="external nofollow noopener" target="_blank">article</a>, which provides a nice explanation and practical Python examples whih helped me along the way.</p> <p>Here‚Äôs how the action distribution looked like after action masking was implemented:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-mask-result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from a training session with MPPO (PPO with action masking). W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi/groups/M8-PBT-MPPO-20231206_113004/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>. </div> <p>The agent was no longer getting stuck in a single-action policy and was instead making meaningful actions: shoot (labelled MOVE7 above) and other attack actions, since all of them were now valid. The agent was now able to decisively win this battle (80% winrate).</p> <h5 id="Ô∏è-hyperparameter-optimization">üéõÔ∏è Hyperparameter optimization</h5> <p>The above results were all achieved with MPPO, however this wasn‚Äôt as simple as it may look. The algorithm has around 10 configurable parameters (called <em>hyperparameters</em>), all of which needed adjustments until the desired result was achieved.</p> <p>Trying to find an optimal combination of hyperparameters is not an easy task. Some of those <em>greatly</em> influence the training process - turning the knob by a hair can result in a completely different outcome. All RL algos have those. While I‚Äôve read that one can develop a pretty good intuition over the years, there‚Äôs certainly no way to be certain which combination of values works best for the task ‚Äì there‚Äôs no silver bullet here. It‚Äôs a trial-and-error process.</p> <p>W&amp;B <a href="https://docs.wandb.ai/guides/sweeps" rel="external nofollow noopener" target="_blank">sweeps</a> are one way to do hyperparameter tuning and I‚Äôve successfully used them in previous RL projects. The early stopping criteria helps save computational power that would be otherwise wasted in poorly performing runs, but newly started runs need to learn everything from scratch. There had to be a better way.</p> <p>That‚Äôs how I learned about the <a href="https://deepmind.google/discover/blog/population-based-training-of-neural-networks/" rel="external nofollow noopener" target="_blank">Population Based Training</a> (PBT) method. It‚Äôs essentially a hyperparameter tuning technique that <em>does not periodically throw away your agent‚Äôs progress</em>. While educating myself on the topic, I stumbled upon the <a href="https://www.anyscale.com/blog/population-based-bandits" rel="external nofollow noopener" target="_blank">Population Based Bandits</a> (PB2) method, which improves upon PBT by leveraging a probabilistic model for selecting the hyperparameter values, so I went for it instead.</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-pbt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of three different approaches for hyperparameter tuning (source: original PBT paper). <br>At regular intervals, PBT (and PB2) will terminate lifelines of poorly performing agents and spawn clones of top-performing ones in their stead. </div> <p>PB2 <a href="https://wandb.ai/s-manolloff/vcmi/reports/Experimenting-with-PB2--Vmlldzo2NzI1ODc4" rel="external nofollow noopener" target="_blank">certainly helped</a>, but this seems mostly a result of its ‚Äúnatural selection‚Äù feature, rather than hyperparameter tuning. In the end, a single policy was propagated to all agents and the RL problem was solved, but huge ranges of hyperparameters remained unexplored. Apart from that, PB2‚Äôs bayesian optimization took took 20% of the entire training iteration time (where 1 iteration was ~5 minutes), so I opted for the vanilla PBT instead. I have used in all vcmi-gym experiments since and basically all results published here are achieved with PBT.</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-optimizers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Hyperparameter optimization in vcmi-gym </div> <p>I must point out that the hyperparameter combination is not static throughought the RL training. It <em>changes</em>. For example, in the early stages of training, lower values for <code class="language-plaintext highlighter-rouge">gamma</code> and <code class="language-plaintext highlighter-rouge">gae_lambda</code> (PPO params) result in faster learning, since the agent considers near-future rewards as more important than far-future rewards. It makes perfect sense if we compare it to how we (humans) learn: starting with the basics, gradually increasing the complexity and coming up with longer-term strategies as we become more familiar with our task.</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-pbt-gamma-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> PBT in action: best performance is achieved when `gamma` and `gae_lambda` start with a lower value, gradually increasing as the agent gains deeper understanding of the environment and is able to efficiently account for long-term rewards. W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi-gym/groups/PBT-v4-pools-20240830_010954/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>. </div> <h5 id="-datasets">üìö Datasets</h5> <p>Achieving an 80% winrate sounds good, but the truth is, the agent was far from intelligent at that point. The obvious problem was that it could only win battles of this (or very similar) type. A different setup (quantities, creature types, stack numbers, etc.) would confuse the agent - the battle would look different compared to everything it was trained for. In other words, the did not generalize well.</p> <p>This is a typical problem in ML and can usually be resolved by ensuring the data used for training is rich and diverse. In the context of vcmi-gym, the ‚Äúdataset‚Äù can be thought of:</p> <ol> <li>Army compositions: creature types, number of stacks and stack quantities.</li> <li>Battlefield terrains: terrain types, number and location of obstacles.</li> <li>Enemy behaviour: actions the enemy takes in a given battle state.</li> </ol> <p>Ensuring as much diversity as possible was a top priority now. The question was <em>how</em> to do it?</p> <p>My initial attempt was to make use of Gym‚Äôs concept for <a href="https://gymnasium.farama.org/api/experimental/vector/" rel="external nofollow noopener" target="_blank">Vector environments</a>, which allows to train on N different environments at once. This means the agent would see N different datasets on each timestep which would provide some degree of data diversity. The problem was that VCMI is designed such that only one instance of the game can run at a time, I had to deal with that first (later submitted it as a standalone <a href="https://github.com/vcmi/vcmi/pull/4253" rel="external nofollow noopener" target="_blank">contribution</a>).</p> <p>With vector environments, my agents had access to a more diverse dataset. This, however, lead to (surprise) another problem: the system RAM became a bottleneck, as all those VCMI instances running in parallel needed ~400MB each. At about 30 VCMI instances, my gear (MacBook M2 pro) started to choke, which meant that was the limit of different battle compositions I could use for training. Turned out it‚Äôs not enough ‚Äì the agent was still unable to generalize well. I needed a <em>lot</em> more than 30 armies.</p> <p>I delved into the VCMI codebase again in search for a way to dynamically change the armies on each restart. Here‚Äôs the what the server-client communication looks like when there‚Äôs a battle:</p> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-replaybattle-sequence-simplified.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The VCMI server-client packet communication sequence at battle (re-)start </div> <p>The <code class="language-plaintext highlighter-rouge">BattleStart</code> server packet is the culprit here. It contains stuff like hero and army IDs, map tile, obstacles, etc. Typically, those don‚Äôt change when a battle is re-played, so I plugged in a server-side function which modifies the packet before it gets sent, setting a pseudo-random tile, obstacle layout and army ID. The thing is, the army ID must refer to an army (i.e. hero) object already loaded in memory. In short, the army must already exist on the map, but my test maps contained only two heroes.</p> <p>Technically, it is possible to generate armies on-the-fly (before each battle), but the problem is: those armies will be <em>imbalanced</em> in terms of strength. This will introduce unwanted noise during training, as agents will often win (or lose) battles not because of their choice of actions, but simply because the armies were vastly different in strength. Poor strategies may get reinforced as they result in a victory which occurred only because the agent‚Äôs army had been too strong to begin with. The only way to ensure armies are balanced is to generate, <em>test</em> and <em>rebalance</em> them beforehand, ensuring all armies get equal chances of winning.</p> <p>This meant I had to see how VCMI maps are generated, write my own map generator, produce map with <em>lots</em> of different armies and then test and rebalance it until it was ready to be used as a training map. It was quite the effort and ultimately resulted in an entire system comprised of several components:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-mapgen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System for generating train (and test) data </div> <p>The cycle goes like this:</p> <ol> <li>Generate a map with many heroes (i.e. armies) of <em>roughly</em> similar strength</li> <li>Evaluate the map by collecting results for each army based on the battles lead by the built-in bot on both sides</li> <li>Rebalance the map by adding/removing units from the armies based on aggregated results from 2.</li> <li>Repeat step 2. and 3. until no further rebalancing is needed</li> </ol> <p>Speaking in numbers, I have been using maps with 4096 heroes, which means there are more than 16M unique parings, making it very unlikely that the agent will ever train with the exact same army compositions twice. There‚Äôs a catch here: in HOMM3, there‚Äôs no way to have 4096 heroes on the map at the same time (the hero limit is 8). Fortunately, VCMI has no such limit, but imposes another restriction: there can‚Äôt be duplicate heroes on the map, and the base game features less than 200 heroes in total. To make this possible, I had to create a VCMI ‚Äúmod‚Äù which adds each of those 4096 heroes as a separate hero type and the issue was gone:</p> <p>That partly solved the issue with the diverse training data. The only issue with this approach was that all armies were of equal strength - for example, all armies were corresponding to a mid-game army (60-90 in-game days). This meant that the agent had less training experience with early-game (weaker) armies, as well as late-game (stronger) armies. I later improved this map generator and added the option to create <code class="language-plaintext highlighter-rouge">pools</code> of heroes, where all heroes within a pool have equal strength, but heroes in different pools have different stregths. Here is how one such map looks like:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-testmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Procedurally generated map with 4096 unique hero armies organized into 4 pools. Here, a hero with ID=85 that belongs to a pool of armies with total <a href="https://heroes.thelazy.net/index.php/AI_value" rel="external nofollow noopener" target="_blank">AI Value</a> of 10K (early-game army pool) is selected. </div> <p>By training on those maps, agents were able to generalize well and maintain steady performance across a wide variety of situations. Below are the results achieved by an agent trained on a 4096-army map and then evaluated on an unseen data set:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-mapgen-results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Agent evaluation results from simulated early-game (20K) and late-game (300K) battles </div> <p>The dataset was unseen in the sense that it included armies of total strength 20K and 300K (neither can be found in the training dataset). The battle terrain and obstacles were not exactly unseen, as there is a finite combination of possible terrains and the agent had probably encountered them all before. The enemy behaviour, however, was also new, since all training is done vs. the scripted ‚ÄúStupidAI‚Äù bot, while half of the evaluation is done vs. ‚ÄúBattleAI‚Äù (VCMI‚Äôs strongest scripted bot).</p> <p>Having said all this, I still haven‚Äôt described how exactly I evaluate the those models, so make sure to read the next section if you want to know more.</p> <h5 id="Ô∏è-testing-evaluation">üïµÔ∏è Testing (evaluation)</h5> <p>Evaluating the trained agents is a convenient way to determine if the training is productive. A typical problem that occurs during training is overfitting: the metrics collected during training are good, but the metrics collected during testing are not.</p> <p>At the time I was training my models on 3 different machines at home (poor man‚Äôs RL training cluster), each of which was exporting a new version of the model being trained at regular intervals. I used W&amp;B as a centralized storage (they give you 100GB of storage on the free tier, which is amazing) for models and and designed an evaluator which could work in a distributed manner (i.e. in parallel with other evaluators on different machines), pulling stored models, evaluating their performance and pushing the results. Here‚Äôs how it looks:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-evaluators.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System for evaluating trained models </div> <p>At regular intervals, the hyperparam optimizer uploads W&amp;B artifacts containing trained models. The evaluators download those models, load them and let them play against multiple types of opponents (StupidAI and BattleAI - the two in-game scripted bots) on several different maps which contain unseen army compositions. The results of those battles are stored for visualisation and the models - marked as evaluated (using W&amp;B artifact tags) to prevent double evaluation.</p> <h5 id="-neural-network">üß† Neural Network</h5> <p>Along with the reward scaling, the thing I experimented the most with was the architectures of the neural network. Various mixtures of fully-connected (FC), convolutional (Conv1d, Conv2d), residual (ResNet), recurrent (LSTM) and self-attention layers were used in this project with varying success. In general, results for NNs which contained SelfAttention and/or LSTM layers were simply worse, so I eventually stopped experimenting with them. Similarly, batch normalization and dropout also seemed to do more harm than good, so those were removed as well.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-fc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-heads.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-v4-lstm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-v4-deep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Some of the NN architectures used in vcmi-gym (there are way too many variations to visualize them all) </div> <p>A slight modification of the NN shown in the last image above seems to perform well and is currently used for training. The NN architecture is, however, often being changed, so it‚Äôs likely the diagrams above are already outdated.</p> <p>.</p> <p>.</p> <p>.</p> <p><i>There‚Äôs certainly more to this story, but I haven‚Äôt had the time to write it all down yet, so stay tuned.</i></p> <p><strong>‚Ä¶ to be continued.</strong></p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2024 Simeon Manolov. <br>Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?939a575388f286729c0279bcadb1d6f8"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>