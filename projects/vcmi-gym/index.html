<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> vcmi-gym | Simeon Manolov </title> <meta name="author" content="Simeon Manolov"> <meta name="description" content="An AI for the game of " heroes of might and magic iii> <meta name="keywords" content="ai-projects, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%95%B9%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smanolloff.github.io/projects/vcmi-gym/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?eb1d88d288af1dac35de430dae5a0768"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Simeon¬†</span> Manolov </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">vcmi-gym</h1> <p class="post-description">An AI for the game of "Heroes of Might and Magic III"</p> </header> <article> <p>You must have heard about <a href="https://en.wikipedia.org/wiki/Heroes_of_Might_and_Magic_III" rel="external nofollow noopener" target="_blank">Heroes of Might and Magic III</a> - a game about <strong>strategy</strong> and <strong>tactics</strong> where you gather resources, build cities, manage armies, explore the map and fight battles in an effort to defeat your opponents. The game is simply awesome and has earned a very special place in my heart.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-adventure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-battle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-town.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ah, just looking at these images makes me want to <i>install the game right now!</i><br>If you know, you know :) </div> <p>Released back in 1999, it‚Äôs still praised by a huge community of fans that keeps creating new game content, expansion sets, gameplay mods, HD remakes and whatnot. That‚Äôs where <a href="https://vcmi.eu/" rel="external nofollow noopener" target="_blank">VCMI</a> comes in: a fan-made <strong>open-source recreation</strong> of HOMM3‚Äôs engine ‚ù§Ô∏è As soon as I heard about it, my eyes sparkled and I knew what my next project was going to be: an AI for HOMM3.</p> <h3 id="problem-statement">Problem Statement</h3> <p>The game features scripted AI opponents which are not good at playing the game and are no match for an experienced human player. To compensate for that, the AI is <em>cheating</em> - i.e. starts with more resources, higher-quality fighting units, fully revealed adventure map, etc. Pretty lame.</p> <h3 id="objective">Objective</h3> <p>Create a non-cheating AI that is better at playing the game.</p> <h3 id="proposed-solution">Proposed Solution</h3> <p>Transform VCMI into a reinforcement learning environment and forge an AI which meets the objective.</p> <p>Contribute to the VCMI project by submitting the pre-trained AI model along with a minimal set of code changes needed for adding an option to enable it via the UI.</p> <h3 id="approach">Approach</h3> <p>Given that the game consists of several distinct player perspectives (combat, adventure map, town and hero management), training separate AI models for each of them, starting with the simplest one, seems like a good approach.</p> <h5 id="phase-1-preparation">Phase 1: Preparation</h5> <ol> <li>Explore VCMI‚Äôs codebase <ul> <li>gather available documentation</li> <li>read, set-up project locally &amp; debug</li> <li>if needed, reach out to VCMI‚Äôs maintainers</li> <li>document as much as possible along the way</li> </ul> </li> <li>Research how to interact with VCMI (C++) from the RL env (Python) <ul> <li>best-case scenario: load C++ libraries into Python and call C++ functions directly</li> <li>fallback scenario: launch C++ binaries as Python sub-processes and use inter-process communication mechanisms to simulate function calls</li> </ul> </li> </ol> <h5 id="phase-2-battle-only-ai">Phase 2: Battle-only AI</h5> <ol> <li>Create a VCMI battle-only RL environment <ul> <li>follow the Farama Gymnasium (ex. OpenAI Gym) API standard</li> <li>optimise VCMI w.r.t. performance (e.g. no UI, fast restarts, etc.)</li> </ul> </li> <li>Train a battle-only AI model <ul> <li>observation design: find the optimal amount of data to extract for at each timestep</li> <li>reward design: find the optimal reward (or punishment) at each timestep</li> <li>train/test data: generate a large and diverse data set (maps, army compositions) to prevent overfitting.</li> <li>start by training against the VCMI scripted AI, then gradually introduce self-training by training vs older versions of the model itself. Optionally, develop a MARL (multi-agent RL) where two two agents are trained concurrently.</li> <li>observability: use W&amp;B and Tensorboard to monitor the training performance</li> </ul> </li> <li>Bundle the pre-trained model into a VCMI mod called ‚ÄúMMAI‚Äù which replaces VCMI‚Äôs default battle AI</li> <li>Contribute to the VCMI project by submitting the MMAI plugin along with the minimal set of changes to the VCMI core needed by the plugin</li> <li>Contribute to the Gymnasium project by submitting vcmi-gym as an official third-party RL environment</li> </ol> <h5 id="phase-3-adventure-only-ai">Phase 3: Adventure-only AI</h5> <p>Worthy of a separate project on its own, training an Adventure AI is out of scope for now. A detailed action plan is not yet required.</p> <h2 id="dev-log">Dev log</h2> <p>I will be outlining parts of the project‚Äôs development lifecycle, focusing on those that seemed most impactful.</p> <p>Since I started this project back in 2023, there will be features in VCMI (and vcmi-gym) that were either introduced, changed or removed since then, as both are actively evolving. Still, most of what is written here should be pretty much accurate for at least a few years time.</p> <h4 id="setting-up-vcmi">Setting up VCMI</h4> <p>First things first - I needed to start VCMI locally in debug mode. Thankfully, the VCMI devs have provided a nice <a href="https://github.com/vcmi/vcmi/blob/develop/docs/developers/Building_macOS.md" rel="external nofollow noopener" target="_blank">guide</a> for that. Some extra steps were needed in my case (most notably due to Qt installation errors), for which I decide to prepare a step-by-step vcmi-gym <a href="https://github.com/smanolloff/vcmi-gym?tab=readme-ov-file#installation" rel="external nofollow noopener" target="_blank">setup guide</a> in the project‚Äôs official git page. the installation notes below:</p> <p>Except for a few hiccups, the process of setting up VCMI went smooth. It was educational and gave me some basic, but useful knowledge:</p> <ul> <li>CMake is a tool for compiling many C++ source files with a single cmake command. A must for any project consisting of more than a few source files.</li> <li> <code class="language-plaintext highlighter-rouge">conan</code> is like <code class="language-plaintext highlighter-rouge">pip</code> for C++</li> <li>Debugging C++ code can be done in many different ways, so I made sure to experiment with a few of them: <ul> <li> <code class="language-plaintext highlighter-rouge">lldb</code> can be used directly as a command-line debugging tool. I feel quite comfortable with CLIs and I looked forward to using this one, but the learning curve was a bit steep for me, so opted for a GUI this time.</li> <li>Sublime Text‚Äôs C++ debugger was a disappointment. I am a <strong>huge</strong> fan of ST and consider it the best text editor out there, but it‚Äôs simply not good for debugging.</li> <li>VSCode‚Äôs debugger really saved the day. Has some glitches, but is really easy to work with. I will always prefer ST over VSC, but I do have it installed at all times, just for its debugger capabilities.</li> <li>Xcode did not work well. VCMI is not a project that does not follow many Xcode conventions (naming, file/directory structure, etc.) which made it hard to simply navigate through the codebase. Well, IDEs have never felt comfortable anyway, so there was no need to waste my time further here.</li> </ul> </li> <li>C++ language feature support (completions, linting, navigation, etc.) in Sublime Text 4 is provided by the <a href="https://github.com/sublimelsp/LSP-clangd" rel="external nofollow noopener" target="_blank">LSP-clangd</a> package and is <em>awesome</em>. I found it vastly superior to VSCode‚Äôs buggy C++ extension.</li> </ul> <p>Unfortunately, VCMI‚Äôs dev setup guide was like a nice welcome-drink on a party where nothing else is included. That‚Äôs to say, there was no useful documentation for me beyond that point. No surprises here - I‚Äôve worked on enough projects where keeping it all documented is a <em>hopeless</em> endeavor, so I don‚Äôt blame anyone. My hope is that the code is well-structured and easy to understand. Good luck with that - the codebase is over 300K lines of C++ code.</p> <h4 id="so-how-does-it-work">So‚Ä¶ how does it work?</h4> <p>Having it up &amp; running, it was time to delve into the nitty-gritty of the VCMI internals and start connecting the dots.</p> <h5 id="vcmis-client-server-communication-protocol">VCMI‚Äôs client-server communication protocol</h5> <p>The big picture is a classic client-server model with a single server (owner of the global game state) and many clients (user interfaces).</p> <p>Communication is achieved via TCP where the application data packets are serialized versions of <code class="language-plaintext highlighter-rouge">CPack</code> objects, for example:</p> <div class="row"> <div class="col-sm-3 offset-sm-2 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-newgame-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-makeaction-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><i>Note: the term ‚ÄúLobby‚Äù in the diagrams refers to the game‚Äôs Main Menu (in this case - ‚ÄúNew Game‚Äù screen) and should not be confused with VCMI‚Äôs lobby component for online multiplayer.</i></p> <p><code class="language-plaintext highlighter-rouge">MakeAction</code>, <code class="language-plaintext highlighter-rouge">StartAction</code>, etc. are sub-classes of the <code class="language-plaintext highlighter-rouge">CPack</code> base class ‚Äì since C++ is a strongly typed language, there‚Äôs a separate class object for each different data structure:</p> <div class="row"> <div class="col-sm-4 offset-sm-4 mt-4 offset-mt-4 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-cpack-wbs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The <code>CPack</code> class tree (SVG version <a href="/assets/img/vcmi-gym/diagram-cpack-wbs.svg" target="_blank">here</a>) </div> <p>Whenever I have to deal with a proprietary network communication protocol which I am not familiar with, I tend to examine the raw data sent over the wire to make sure I am not missing anything. So I sniffed a couple of TCP packets using Wireshark and here‚Äôs what the raw data looks like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== MakeAction command packet. Format:
==
== [32-bit hex dump] // [value in debugger] // [dtype] // [field desc]

01           // \x01  ui8   hlp (true, i.e. not null)
FA 00        // 250   ui16  tid (typeid)
00           // \0    ui8   playerColor
29 00 00 00  // 41    si32  requestID
00           // 0     ui8   side
00 00 00 00  // 0     ui32  stackNumber
02 00 00 00  // 2     si32  actionType
FF FF FF FF  // -1    si32  actionSubtype
01 00 00 00  // 1     si32  length of (unitValue, hexValue) tuples
18 FC FF FF  // -1000 si32  unitValue (INVALID_UNIT_ID)
45 00        // 69    si16  hexValue.hex
</code></pre></div></div> <p>The observed hex dump aligns with (parts) of the object information displayed in the debugger, as well the class declarations for <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/NetPacks.h#L2500" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MakeAction</code></a>, its member variable of type <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/battle/BattleAction.h#L24" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">BattleAction</code></a> field and its parent <a href="https://github.com/vcmi/vcmi/blob/1.3.2/lib/NetPacksBase.h#L95" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CPackForServer</code></a>. The communication protocol between the server and the clients is now clear.</p> <p>However, the data itself does not tell much about the system‚Äôs behaviour. There‚Äôs a lot going on after that data is received ‚Äì let‚Äôs see what.</p> <h5 id="data-processing">Data processing</h5> <p>Any packet that is accepted by a client or server essentially spins a bunch of gears which ultimately change the global game state and, optionally, result in other data packets being sent in response.</p> <p>Figuring out the details by navigating through the VCMI codebase is nearly impossible with the naked eye ‚Äì there are ~300K lines of code in there. That‚Äôs where the debugger really comes in handy ‚Äì with it, I followed the code path of the received data from start to end and mapped out the important processing components involved. Digging deeper into the example above, I visualized the handling of a <code class="language-plaintext highlighter-rouge">LobbyClientConnected</code> packet from two different perspectives in an attempt to get a grip on what‚Äôs going on:</p> <div class="row"> <div class="col-sm-2 offset-sm-1 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-lobbyclientconnected-activity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-lobbyclientconnected-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <code>LobbyClientConnected</code> server packet handling (SVG versions <a href="/assets/img/vcmi-gym/diagram-lobbyclientconnected-activity.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-lobbyclientconnected-routing.svg" target="_blank">here</a>) </div> <p>That‚Äôs the ‚Äúshort‚Äù version, anyway ‚Äì many irrelevant details and function calls are omitted from the diagrams to keep it readable. But it‚Äôs just one of many possible codepaths, considering the number of different packets (see the <code class="language-plaintext highlighter-rouge">CPack</code> class tree above). It would be impossible to map them all, but I did map few more as I kept exploring the codebase ‚Äì I am sharing them here as they might come in handy in the future:</p> <div class="row"> <div class="col-sm-4 offset-sm-1 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-heromoved-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-2 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-playerblocked-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-makeaction-routing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> More packet routing diagrams (SVG versions: <a href="/assets/img/vcmi-gym/diagram-heromoved-routing.svg" target="_blank">here</a>, <a href="/assets/img/vcmi-gym/diagram-playerblocked-routing.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-makeaction-routing.svg" target="_blank">here</a>) </div> <p>Patterns of the processing logic began to emerge at that point, meaning I had reached a satisfactory level of general understanding about how VCMI works. It was time to think about transforming it into a reinforcement learning environment.</p> <h4 id="optimizing-vcmi-for-rl">Optimizing VCMI for RL</h4> <p>VCMI‚Äôs user-oriented design makes it unsuitable for training AI models efficiently. On-policy RL algorithms like PPO are designed to operate in environments where state observations can collected at high speeds and training a battle AI is a process that will involve a <em>lot</em> of battles being played. We are talking millions here.</p> <h5 id="quick-battles">Quick battles</h5> <p>With the animation speed set to max and auto-combat enabled, a typical 10 round battle takes around a minute. <a href="https://www.youtube.com/watch?v=hV_2Q-sjYOA" rel="external nofollow noopener" target="_blank"><em>Ain‚Äôt nobody got time for that</em></a>.</p> <p>The game features a ‚Äúquick combat‚Äù setting which causes battles to be carried out in the background without any user interaction (the user‚Äôs troops are controlled by the computer instead). With quick combat enabled, a battle gets resolved in under a second, which is a great improvement. Clearly, combat <strong>training should be conducted in the form of quick combats</strong>.</p> <p>But what good is a sub-second combat if it takes 10+ seconds to restart?</p> <h5 id="quick-restarts">Quick restarts</h5> <p>Fortunately, VCMI features the potentially helpful ‚Äúquick combat replays‚Äù setting. Sadly, it allows only a <em>single manual replay</em> per battle and only when the enemy is a neutral army - not particularly useful in my case. What I need is <em>infinite quick replays</em>.</p> <p>A deeper look into VCMI‚Äôs internals reveals the query stack, where each item roughly corresponds to an event whose outcome depends on other events which might occur in the meantime. When the player chooses to restart combat, the results from battle query #2 are not applied and the entire query is re-inserted back in the stack (with a few special flags set).</p> <div class="row"> <div class="col-sm-7 offset-sm-1 mt-7 offset-mt-1 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/querystack.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-replaybattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The VCMI query stack and a communication diagram for (re-)starting combat (SVG <a href="/assets/img/vcmi-gym/diagram-replaybattle-sequence.svg" target="_blank">here</a>) </div> <p>Removing the restart battle restrictions involved relatively minor code changes and even revealed a small <a href="https://github.com/vcmi/vcmi/issues/953#issuecomment-1787151606" rel="external nofollow noopener" target="_blank">memory leak</a> in VCMI itself (at least for VCMI v1.3.2, it should already be fixed in v1.4+)</p> <h5 id="benchmarks">Benchmarks</h5> <p>My poor man‚Äôs benchmark setup consists of a simple 2-player micro adventure map (2x2) where two opposing armies of similar strength (heroes with 7 groups of units each) engage in a battle which is restarted immediately after it ends:</p> <div class="row justify-content-md-center"> <div class="col-sm-8 mt-8 offset-mt-2 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/testmap-layout.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A very simple map for testing purposes </div> <p>The benchmark was done on my own laptop (M2 Macbook Pro). Here are the measurements:</p> <ul> <li>16.28 battles per per second</li> <li>862 actions per second (431 per side)</li> </ul> <p>The results were good enough for me - there was no need to optimize VCMI further at this point. It was about time I started thinking on how to integrate it with the Python environment.</p> <h4 id="embedding-vcmi">Embedding VCMI</h4> <p>The <a href="https://gymnasium.farama.org/" rel="external nofollow noopener" target="_blank">Farama Gymnasium</a> (formerly OpenAI Gym) API standard is a Python library that aims to make representing RL problems easier. I like it because of its simplicity and wide adoption rate within the python RL community (RLlib, StableBaselines, CleanRL are a few examples), so I going to use it for my RL environment.</p> <p>Communicating with a C++ program (i.e. VCMI) from a Python program was a new and exciting challenge for me. Given that the Python interpreter itself is written in C++, it had to be possible. I googled a bit and decided to go with <a href="https://pybind11.readthedocs.io/en/stable/" rel="external nofollow noopener" target="_blank">pybind11</a>, which looked like the tool for the job.</p> <p>I quickly ran into various issues related to memory violations. Anyone that has worked with data pointers (inevitable in C++) has certainly encountered the infamous <em>Undefined Behaviour‚Ñ¢</em> monster that can make a serious mess out of any program. Turns out there is a very strict line one must not cross when embedding C++ in Python: accessing data outside of a python thread without having acquired the Global Interpreter Lock (GIL). A Python developer never really needs to think about it until they step out of Python Wonderland and enter the dark dungeons of a C++ extension. I managed to eventually get the hang of it and successfully integrated both programs. A truly educational experience.</p> <p>While experimenting with pybind11, I was surprised to find out VCMI can‚Äôt really be <em>embedded</em> as it refuses to boot in a non-main thread. Definitely a blocker, since the main thread during training is the RL script, not VCMI. Bummer.</p> <p>A quick investigation revealed that the <a href="https://www.libsdl.org/" rel="external nofollow noopener" target="_blank">SDL</a> loop which renders the graphical user interface (GUI) was responsible for the issue. This GUI had to go.</p> <h5 id="removing-the-gui">Removing the GUI</h5> <p>There have always been many reasons to remove the GUI ‚Äì it is not used during training, consumes additional hardware resources and enforces a limit on the overall game speed due to hard-coded framerate restrictions, so I was more than happy to deal with it now that it became necessary.</p> <p>The VCMI executable accepts a <code class="language-plaintext highlighter-rouge">--headless</code> flag which causes a runtime error as soon as the game is started. Still, the codebase did contain code paths for running in such a mode, so making it work properly should be an easy win. In the end, I decided to introduce a new build target which defines a function which only <em>initializes</em> the SDL (this is required for the game to run) and another function which starts the game but <em>without</em> activating the SDL render loop.</p> <p>However, with no GUI, it was hard to see what‚Äôs going on, so I ended up coding a text-based renderer which is pretty useful for visualizing battlefield state in the terminal:</p> <div class="row"> <div class="col"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-ansi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> My ANSI text renderer for VCMI \o/ </div> <p>After pleasing the eye with such a result, it was time to get back to the VCMI-Python integration with pybind11.</p> <h5 id="connecting-the-pieces">Connecting the pieces</h5> <p>I refrained from the quick-and-dirty approach even for PoC purposes as it meant polluting with pybind11 code and dependencies all over the place. A separate component had to be designed for the purpose.</p> <p>Typically, connecting two components with incompatible interfaces involves an adapter (I call it <em>connector</em>) which provides a clean API to each of the components:</p> <div class="row justify-content-md-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povgym-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povconnector-components.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povvcmi-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>When applying the <a href="https://refactoring.guru/design-patterns/adapter" rel="external nofollow noopener" target="_blank">adapter pattern</a> one must flip their perspective from the local viewpoint of an object in a relationship, to the shared viewpoint of the relationship itself (i.e. both sides of our connector). That‚Äôs when one issue becomes apparent: both components are controlled by <em>different</em> entities - i.e. the VCMI client receives input from the VCMI Server, while the gym env receives input from the RL algorithm.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-povconnector-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A view on the vcmi-gym relationships through the connector </div> <p>Since both controlling entities (RL script and VCMI server) are otherwise unrelated, the <em>connector</em> is responsible for ensuring that they operate in a mutually synchronous manner. The solution involves usage of synchronization primitives to block the execution of one thread while the other is unblocked:</p> <div class="row justify-content-md-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-init-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-step-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-reset-endbattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-connector-reset-midbattle-sequence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Connector implementation details (SVG versions <a href="/assets/img/vcmi-gym/diagram-connector-init-sequence.svg" target="_blank">here</a>, <a href="/assets/img/vcmi-gym/diagram-connector-step-sequence.svg" target="_blank">here</a> <a href="/assets/img/vcmi-gym/diagram-connector-reset-endbattle-sequence.svg" target="_blank">here</a> and <a href="/assets/img/vcmi-gym/diagram-connector-reset-midbattle-sequence.svg" target="_blank">here</a>) </div> <p>Some notes regarding the diagrams above:</p> <ul> <li>The gray background denotes a group of actors operating within the same thread (where <code class="language-plaintext highlighter-rouge">T1</code>, <code class="language-plaintext highlighter-rouge">T2</code>, ‚Ä¶ are the threads). The same actor can operate in multiple threads.</li> <li>The meaning behind the color-coded labels is as follows: <ul> <li> <span style="background-color: yellow; color: black">acquire lock</span>: a successful attempt to acquire the shared lock. The can be released explicitly via <span style="background-color: black; color: yellow">release lock</span> or implicitly at the end of the current call block.</li> <li> <span style="background-color: yellow; color: red">acquire lock</span>: an unsuccessful attempt to acquire the shared lock, effectively blocking the current thread execution until the lock is released.</li> <li> <span style="background-color: red; color: black">cond.wait</span>: the current thread execution is blocked until another thread notifies it via <span style="color: blue">cond.notify</span> (<code class="language-plaintext highlighter-rouge">cond</code> is a <a href="https://en.cppreference.com/w/cpp/thread/condition_variable" rel="external nofollow noopener" target="_blank">conditional variable</a>).</li> <li> <span style="color: gray">P_Result</span> and <span style="color: gray">Action</span>: shared variables modified by reference (affecting both threads)</li> </ul> </li> <li>The red bars indicate that the thread execution is blocked.</li> <li> <code class="language-plaintext highlighter-rouge">AAI</code> and <code class="language-plaintext highlighter-rouge">BAI</code> are the names of my C++ classes which implement VCMI‚Äôs <code class="language-plaintext highlighter-rouge">CAdventureAI</code> and <code class="language-plaintext highlighter-rouge">CBattleGameInterface</code> interfaces. Each VCMI AI (even the default scripted one) defines such classes.</li> <li> <code class="language-plaintext highlighter-rouge">baggage</code> is a special struct which contains a reference to the <code class="language-plaintext highlighter-rouge">GetAction</code> function. I introduced the Baggage idiom in VCMI to enable <a href="https://en.wikipedia.org/wiki/Dependency_injection" rel="external nofollow noopener" target="_blank">dependency injection</a> - specifically, the Baggage struct is seen as a simple <code class="language-plaintext highlighter-rouge">std::any</code> object by upstream code, all the way up until <code class="language-plaintext highlighter-rouge">AAI</code> where it‚Äôs converted back to a Baggage struct and the function references stored within are extracted. A lot of it feels like <em>black magic</em> and implementing it was a real challenge (function pointers in C++ are really weird).</li> <li>at the end of a battle, a special flag is returned with the result which indicates that the next action must be <code class="language-plaintext highlighter-rouge">reset</code> </li> <li>in the middle of a battle, a <code class="language-plaintext highlighter-rouge">reset</code> is still a valid action which is effectively translated to <em>retreat</em> + an a ‚Äúyes‚Äù answer to the ‚ÄúRestart battle‚Äù dialog</li> </ul> <p>With that, the important parts of the VCMI-gym integration were now in place. There were many other changes which I won‚Äôt discuss here, such as dynamic TCP port allocation, logging improvements, fixes for race conditions when starting multiple VCMIs, building and loading it as a single shared dynamic library, etc. My hands already itched to work on the actual RL part which was just over the corner.</p> <h3 id="renforcement-learning">Renforcement Learning</h3> <p>This is the part I found the most difficult. While there are well-known popular RL algos out there, it‚Äôs not a simple matter of <em>plug&amp;play</em> (or <em>plug&amp;train</em> :)) It‚Äôs a lengthy process involving a cycle of of research, imeplentation, deployment, observation and analysis. It‚Äôs why I am always taken aback by my friends‚Äô reactions when telling them about this project:</p> <p>‚ÄúThe AI algorithms have already been developed, what takes you so long?‚Äù</p> <p>There‚Äôs so much I have to say here, but most often, I prefer to avoid talking for another hour about it, so I usually pick an answer as short as ‚ÄúTrust me, it‚Äôs not‚Äù. And it‚Äôs not a good answer, I know that. Call it bad marketing - not just my answer, about this project, but the ubiquitous hype around AI that has made people thinking <i>‚ÄúThat‚Äôs it, humanity has found the <em>formula for AI</em>. Now it‚Äôs all about having a good hardware‚Äù</i>. Well, if that were the case, I wouldn‚Äôt bother with this article in the first place. If you are interested in the long answer, read ahead.</p> <h4 id="the-rl-cycle">The RL cycle</h4> <p>There‚Äôs plenty of information about RL on the web, but I will outline the most essential part it: the action-steate-reward cycle which nicely fits into this simple diagram:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-diagram.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So far I had prepared VCMI as an RL environment, but that‚Äôs just one key piece. Next up is presenting its state to the agent.</p> <h5 id="Ô∏è-observations">üëÅÔ∏è Observations</h5> <p>Figuring out which parts of the environment state should be ‚Äúobservable‚Äù by the agent (i.e. the observation space) is a balance between providing enough, yet not too much information.</p> <p>I prefer the <a href="https://en.wikipedia.org/wiki/Empathic_design" rel="external nofollow noopener" target="_blank">Empathic design</a> approch when dealing with such a problem, trying to imagine myself playing the game (and, if possible, actually play it) with very limited information about its current state, taking notes of what‚Äôs missing and how important is it. For example, playing a game of Chess without seeing the pieces that have been taken out, or without seeing the enemy‚Äôs remaining time is OK, but things get rough if I can‚Äôt distinguish the different types of pieces, for example.</p> <p>Since the agent‚Äôs observation space is just a bunch of numbers organized in vectors, it would be nearly impossible for me to interpret it directly ‚Äì rather, I apply certain post-processing to the observation and transform it into something that my brain can use (yes, that is a sloppy way to describe ‚Äúuser interface‚Äù). The important part is this: all information I get to see is simply a projection of the information the agent gets to see.</p> <p>The observation space went through several design iterations, but I will only focus on the latest one.</p> <p>The mandatory component of the observation, the ‚Äúchess board‚Äù equivalent here is the battlefield‚Äôs terrain layout. I started out by enumerating all relevant battle hexes that would represent the basis of the observation. The enumeration looks similar to the one used by VCMI‚Äôs code, but is different.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/hexes.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Each hex can be in exactly one of four states w.r.t. the currently active unit:</p> <ol> <li>terrain obstacle</li> <li>occupied by a unit</li> <li>free (unreachable)</li> <li>free (reachable)</li> </ol> <p>This state is represented by a single number. In addition, there are 15 more numbers which represent the occupying creature‚Äôs attirbutes, similar to what the player would see when right-clicking on the creature: owner, quantity, creature type, attack, defence, etc.</p> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-lizardmen.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5"> <figure> <picture> <img src="/assets/img/vcmi-gym/table-observationspace.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>No information about the terrain‚Äôs type (grass, snow, etc.) as well as the creature‚Äôs morale, luck, abilities and status effects is provided to the agent. A cardinal sin of RL environment design is to ignore the <a href="https://en.wikipedia.org/wiki/Markov_property" rel="external nofollow noopener" target="_blank">Markov property</a>, whereby RL algorithms would struggle to optimize the policies, so it was important to not simply hide those attributes, but take them out of the equation:</p> <ul> <li>terrain is always ‚Äúcursed ground‚Äù (negates morale and luck)</li> <li>heroes have no spell books as well as no spellcasting units in their army</li> <li>heroes have no passive skills or artifacts affecting the units‚Äô stats</li> </ul> <p>The observation space could be expanded and the restrictions - removed as soon as the agent learns to play well enough.</p> <p><i>Note: this is already the case, and additional info was eventually added to the observation: morale, luck, a subset of the creature abilities, total army strength, etc.</i></p> <h5 id="Ô∏è-actions">üïπÔ∏è Actions</h5> <p>Designing the action space is definitely simpler as the agent should be able to perform any action as long as it‚Äôs possible under certain conditions. The only meaningful restrictions here are those that prevent quitting, retreating and (as discussed earlier) spell casting.</p> <p>The total number of possible actions then becomes <strong>2312</strong>, which is a sum of:</p> <ul> <li>1 ‚ÄúDefend‚Äù action</li> <li>1 ‚ÄúWait‚Äù action</li> <li>165 ‚ÄúMove‚Äù actions (1 per hex)</li> <li>165 ‚ÄúRanged attack‚Äù actions (1 per hex)</li> <li>165*12=1320 ‚ÄúMelee attack‚Äù actions (12 per hex - see image below)</li> </ul> <div class="row justify-content-md-center"> <div class="col-sm-3"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/h3-attack-hexes-3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The 12 melee attacks per hex (6 to 11 are available to 2-hex attackers only) </div> <p>The fact is, most of those actions are only <em>conditionally available</em> at the current timestep. For example, the 1652 actions are reduced to around 600 if the active unit has a speed of <code class="language-plaintext highlighter-rouge">5</code>, as it simply can‚Äôt reach most of the hexes. If there are also no enemy units it can target, that number is further reduced to ~200 as the ‚Äúmelee attack‚Äù actions become unavailable. If it‚Äôs a melee unit, the ‚Äúranged attack‚Äù actions also become unavailable, reducing the possible actions to around 50 ‚Äì <em>orders of magnitute</em> lower.</p> <p>Having such a small fraction of valid actions could hinder the initial stage of the learning process, as the agent will be unlikely to select those actions. Introducing action masking can help with this problem ‚Äì more on that later.</p> <h5 id="-rewards">üç© Rewards</h5> <p>Arguably the hardest part is deciding when and how much to reward (or punish) agents for their actions. Sparse rewards, i.e. rewards only at the end of the episode, or battle, may lead to (much) slower learning, while too specific rewards (at every step, based on the particular action taken) may induce too much bias, ultimately preventing the agent from developing strategies which the reward designer did not account for.</p> <p>The reward can be expressed as:</p> \[R = \sum_{i=1}^nS_i(5D_{i} - V_i\Delta{Q_i})\] <p>where:</p> <ul> <li> <strong>R</strong> is the reward</li> <li> <strong>n</strong> is the number of stacks on the battlefield</li> <li> <strong>S<sub>i</sub></strong> is <code class="language-plaintext highlighter-rouge">1</code> if the stack is friendly, <code class="language-plaintext highlighter-rouge">-1</code> otherwise</li> <li> <strong>D<sub>i</sub></strong> is the damage dealt by the stack</li> <li> <strong>V<sub>i</sub></strong> is the stack‚Äôs <a href="https://heroes.thelazy.net/index.php/List_of_creatures" rel="external nofollow noopener" target="_blank">AI Value</a> </li> <li> <strong>ŒîQ<sub>i</sub></strong> is the change in the stack‚Äôs quantity (number of creatures)</li> </ul> <p>And a (simpler) alternative to the mathematical formulation above:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>R = 0
for stack in all_stacks:
    points = 5 * stack.dmg_dealt - (stack.ai_value * stack.qty_diff)
    sign = stack.is_friendly ? 1 : -1
    R += sign * points
</code></pre></div></div> <p>The bottom line is: all rewards are zero-sum, i.e. for each reward point given to one side, a corresponding punishment point is given to the other side.</p> <p>An important note here is that there‚Äôs no punishment for invalid actions (or <em>conditionally unavailable</em> actions) ‚Äì the initial reward design did include such, but it became redundant after I added action masking (see next section).</p> <p><i>Note: this reward function became more complex as the project evolved and eventually included damping factors, fixed per-step rewards, scaling based on total starting army value, etc. An essential part of it still corresponds to the above formula, though.</i></p> <h4 id="Ô∏è-training-algorithm">üèãÔ∏è Training algorithm</h4> <p>I started off with <a href="https://github.com/DLR-RM/stable-baselines3" rel="external nofollow noopener" target="_blank">stable-baselines3</a>‚Äôs PPO and DQN implementations, which I have used in the past, and launched a training session where the left-side army is controlled by the agent and the enemy (right-side army) is controlled by the built-in ‚ÄúStupidAI‚Äù bot:</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-battle-simple.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The idea was to eventually add more units to each army which would progressively increase the complexity.</p> <p>However, one issue quickly popped up: the action space was too big (2000+ actions), while the average number of <em>allowed</em> actions at any given timestep was less than 50. It means an untrained agent has only 2% chance to make an valid action (the initial policy is basically an RNG). The agent was making too many invalid actions and, although it eventually learned to stop making them, it still performed very poorly:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-issue-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The number of invalid actions ("errors") decreases, but the success rate (games won) stays at 0. </div> <p>Something was clearly wrong here. Why was the agent unable to win a game? Clearly it <em>was learning</em>: the <code class="language-plaintext highlighter-rouge">errors</code> chart shows it eventually stops making invalid actions. I needed more information.</p> <p>I started reporting aggregated metrics with the actions being taken and visualised them as a heatmap (had to write a custom <a href="https://vega.github.io/vega/" rel="external nofollow noopener" target="_blank">vega</a> chart in W&amp;B for that). The problem was revealed immediately: the agent‚Äôs policy was converging to a single action:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-issue-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Heatmaps of the agent's attempted actions throughout the training session. The agent learns to make only "Defend" actions. W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi/groups/M2-PPO-20231127_192222/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>). </div> <p>Apparently, all the agent ‚Äúlearned‚Äù was that any action except the ‚ÄúDefend‚Äù action results in a negative reward (recall that 98% of the actions are invalid at any given timestep). While it does yield a higher reward, it is still a poor strategy that will always lead to a loss. This is referred to as <em>local minima</em> and such a state is usually nearly impossible to recover from.</p> <p>One option was to try and tweak the negative feedback for invalid actions. Making it too small or completely removing it caused another local minima: convergence into a policy where only invalid actions were taken, as the episode never finished and thus - the enemy could never inflict any damage (the enemy is not given a turn until the agent finishes its own turn by making a valid move). Maybe I should introduce a negative feedback on ‚ÄúDefend‚Äù actions‚Ä¶? That would introduce too much bias (defending is not an inherently <em>bad</em> action, but the agent would perceive it as such).</p> <p>Reshaping the reward signal was not productive here. I had to prevent such actions from being taken in the first place. To do this, I needed to understand how agents choose their actions:</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-softmax.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>The output layer of a neural network produces a group of <code class="language-plaintext highlighter-rouge">n</code> values (called <em>logits</em>): <code class="language-plaintext highlighter-rouge">[s‚ÇÅ, s‚ÇÇ, ..., s‚Çô]</code>, where <code class="language-plaintext highlighter-rouge">n</code> is the number of actions. The logits have no predefined range, i.e. <code class="language-plaintext highlighter-rouge">s·µ¢ ‚àà (-‚àû, ‚àû)</code>.</li> <li>A <code class="language-plaintext highlighter-rouge">softmax</code> opration forms a discrete <em>probability distribution</em> <code class="language-plaintext highlighter-rouge">P</code>, i.e. the values are transformed into <code class="language-plaintext highlighter-rouge">[p‚ÇÅ, p‚ÇÇ, ..., p‚Çô]</code> where <code class="language-plaintext highlighter-rouge">p·µ¢ ‚àà [0, 1]</code> and <code class="language-plaintext highlighter-rouge">sum(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô) = 1</code>.</li> <li>Depending on the policy, either of the following comes next: <ul> <li>Greedy policy: an <code class="language-plaintext highlighter-rouge">argmax(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code> operation (denoted as <code class="language-plaintext highlighter-rouge">max(p·µ¢)</code> in the figure above) returns the <em>index</em> <code class="language-plaintext highlighter-rouge">i</code> of the value <code class="language-plaintext highlighter-rouge">p·µ¢ = max(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code>, i.e. the action with the highest probability.</li> <li>Stochastic policy: a <code class="language-plaintext highlighter-rouge">sample(p‚ÇÅ, p‚ÇÇ, ..., p‚Çô)</code> operation (not shown in the figure above) returns the <em>index</em> <code class="language-plaintext highlighter-rouge">i</code> of a sampled value <code class="language-plaintext highlighter-rouge">p·µ¢ ~ P</code>.</li> </ul> </li> </ol> <p>Armed with that knowledge, it‚Äôs clear that if the output logits are infinitely small, the probability of their corresponding actions will be <code class="language-plaintext highlighter-rouge">0</code>. This is called <strong>action masking</strong> and is exactly what I needed.</p> <p>Action masking in PPO was already supported in stable-baselines3‚Äôs <a href="https://github.com/Stable-Baselines-Team/stable-baselines3-contrib" rel="external nofollow noopener" target="_blank">contrib</a> package, but was not readily available for any other algorithm. If I wanted such a feature for, say, QRDQN, I was to implement it myself. This turned out to be quite difficult in stable-baselines3, so I decided to search for other RL algorithm implementations which would be more easily extensible. That‚Äôs how I found <a href="https://github.com/vwxyzjn/cleanrl" rel="external nofollow noopener" target="_blank">cleanrl</a>.</p> <p>CleanRL is a true gem when it comes to python implementations of RL algorithms: the the code is simple, concise, self-contained, easy to use and customize, excellent for both practical and educational purposes. It made it easy for me to add action masking to PPO and QRDQN (an RL algorithm which I implemented on top of cleanrl‚Äôs DQN). Prepending an ‚ÄúM‚Äù to those algos was my way of marking them as augmented with action masking (MPPO, MQRDQN, etc).</p> <p>A detailed analysis of the effects of action masking can be found in this <a href="https://arxiv.org/pdf/2006.14171" rel="external nofollow noopener" target="_blank">paper</a>. I would also recommend this <a href="https://boring-guy.sh/posts/masking-rl/" rel="external nofollow noopener" target="_blank">article</a>, which provides a nice explanation and practical Python examples whih helped me along the way.</p> <p>Here‚Äôs how the action distribution looked like after action masking was implemented:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-invalid-action-mask-result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from a training session with MPPO (PPO with action masking). W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi/groups/M8-PBT-MPPO-20231206_113004/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>. </div> <p>The agent was no longer getting stuck in a single-action policy and was instead making meaningful actions: shoot (labelled MOVE7 above) and other attack actions, since all of them were now valid. The agent was now able to decisively win this battle (80% winrate).</p> <h5 id="Ô∏è-hyperparameter-optimization">üéõÔ∏è Hyperparameter optimization</h5> <p>The above results were all achieved with MPPO, however this wasn‚Äôt as simple as it may look. The algorithm has around 10 configurable parameters (called <em>hyperparameters</em>), all of which needed adjustments until the desired result was achieved.</p> <p>Trying to find an optimal combination of hyperparameters is not an easy task. Some of those <em>greatly</em> influence the training process - turning the knob by a hair can result in a completely different outcome. All RL algos have those. While I‚Äôve read that one can develop a pretty good intuition over the years, there‚Äôs certainly no way to be certain which combination of values works best for the task ‚Äì there‚Äôs no silver bullet here. It‚Äôs a trial-and-error process.</p> <p>W&amp;B <a href="https://docs.wandb.ai/guides/sweeps" rel="external nofollow noopener" target="_blank">sweeps</a> are one way to do hyperparameter tuning and I‚Äôve successfully used them in previous RL projects. The early stopping criteria helps save computational power that would be otherwise wasted in poorly performing runs, but newly started runs need to learn everything from scratch. There had to be a better way.</p> <p>That‚Äôs how I learned about the <a href="https://deepmind.google/discover/blog/population-based-training-of-neural-networks/" rel="external nofollow noopener" target="_blank">Population Based Training</a> (PBT) method. It‚Äôs essentially a hyperparameter tuning technique that <em>does not periodically throw away your agent‚Äôs progress</em>. While educating myself on the topic, I stumbled upon the <a href="https://www.anyscale.com/blog/population-based-bandits" rel="external nofollow noopener" target="_blank">Population Based Bandits</a> (PB2) method, which improves upon PBT by leveraging a probabilistic model for selecting the hyperparameter values, so I went for it instead.</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-pbt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of three different approaches for hyperparameter tuning (source: original PBT paper). <br>At regular intervals, PBT (and PB2) will terminate lifelines of poorly performing agents and spawn clones of top-performing ones in their stead. </div> <p>PB2 <a href="https://wandb.ai/s-manolloff/vcmi/reports/Experimenting-with-PB2--Vmlldzo2NzI1ODc4" rel="external nofollow noopener" target="_blank">certainly helped</a>, but this seems mostly a result of its ‚Äúnatural selection‚Äù feature, rather than hyperparameter tuning. In the end, a single policy was propagated to all agents and the RL problem was solved, but huge ranges of hyperparameters remained unexplored. Apart from that, PB2‚Äôs bayesian optimization took took 20% of the entire training iteration time (where 1 iteration was ~5 minutes), so I opted for the vanilla PBT instead. I have used in all vcmi-gym experiments since and basically all results published here are achieved with PBT.</p> <div class="row justify-content-md-center"> <div class="col-sm-6"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-optimizers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Hyperparameter optimization in vcmi-gym </div> <p>I must point out that the hyperparameter combination is not static throughought the RL training. It <em>changes</em>. For example, in the early stages of training, lower values for <code class="language-plaintext highlighter-rouge">gamma</code> and <code class="language-plaintext highlighter-rouge">gae_lambda</code> (PPO params) result in faster learning, since the agent considers near-future rewards as more important than far-future rewards. It makes perfect sense if we compare it to how we (humans) learn: starting with the basics, gradually increasing the complexity and coming up with longer-term strategies as we become more familiar with our task.</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-pbt-gamma-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> PBT in action: best performance is achieved when `gamma` and `gae_lambda` start with a lower value, gradually increasing as the agent gains deeper understanding of the environment and is able to efficiently account for long-term rewards. W&amp;B link <a href="https://wandb.ai/s-manolloff/vcmi-gym/groups/PBT-v4-pools-20240830_010954/workspace?nw=nwusersmanolloff" rel="external nofollow noopener" target="_blank">here</a>. </div> <h5 id="-datasets">üìö Datasets</h5> <p>Achieving an 80% winrate sounds good, but the truth is, the agent was far from intelligent at that point. The obvious problem was that it could only win battles of this (or very similar) type. A different setup (quantities, creature types, stack numbers, etc.) would confuse the agent - the battle would look different compared to everything it was trained for. In other words, the did not generalize well.</p> <p>This is a typical problem in ML and can usually be resolved by ensuring the data used for training is rich and diverse. In the context of vcmi-gym, the ‚Äúdataset‚Äù can be thought of:</p> <ol> <li>Army compositions: creature types, number of stacks and stack quantities.</li> <li>Battlefield terrains: terrain types, number and location of obstacles.</li> <li>Enemy behaviour: actions the enemy takes in a given battle state.</li> </ol> <p>Ensuring as much diversity as possible was a top priority now. The question was <em>how</em> to do it?</p> <p>My initial attempt was to make use of Gym‚Äôs concept for <a href="https://gymnasium.farama.org/api/experimental/vector/" rel="external nofollow noopener" target="_blank">Vector environments</a>, which allows to train on N different environments at once. This means the agent would see N different datasets on each timestep which would provide some degree of data diversity. The problem was that VCMI is designed such that only one instance of the game can run at a time, I had to deal with that first (later submitted it as a standalone <a href="https://github.com/vcmi/vcmi/pull/4253" rel="external nofollow noopener" target="_blank">contribution</a>).</p> <p>With vector environments, my agents had access to a more diverse dataset. This, however, lead to (surprise) another problem: the system RAM became a bottleneck, as all those VCMI instances running in parallel needed ~400MB each. At about 30 VCMI instances, my gear (MacBook M2 pro) started to choke, which meant that was the limit of different battle compositions I could use for training. Turned out it‚Äôs not enough ‚Äì the agent was still unable to generalize well. I needed a <em>lot</em> more than 30 armies.</p> <p>I delved into the VCMI codebase again in search for a way to dynamically change the armies on each restart. Here‚Äôs the what the server-client communication looks like when there‚Äôs a battle:</p> <div class="row justify-content-md-center"> <div class="col-sm-4"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-replaybattle-sequence-simplified.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The VCMI server-client packet communication sequence at battle (re-)start </div> <p>The <code class="language-plaintext highlighter-rouge">BattleStart</code> server packet is the culprit here. It contains stuff like hero and army IDs, map tile, obstacles, etc. Typically, those don‚Äôt change when a battle is re-played, so I plugged in a server-side function which modifies the packet before it gets sent, setting a pseudo-random tile, obstacle layout and army ID. The thing is, the army ID must refer to an army (i.e. hero) object already loaded in memory. In short, the army must already exist on the map, but my test maps contained only two heroes.</p> <p>Technically, it is possible to generate armies on-the-fly (before each battle), but the problem is: those armies will be <em>imbalanced</em> in terms of strength. This will introduce unwanted noise during training, as agents will often win (or lose) battles not because of their choice of actions, but simply because the armies were vastly different in strength. Poor strategies may get reinforced as they result in a victory which occurred only because the agent‚Äôs army had been too strong to begin with. The only way to ensure armies are balanced is to generate, <em>test</em> and <em>rebalance</em> them beforehand, ensuring all armies get equal chances of winning.</p> <p>This meant I had to see how VCMI maps are generated, write my own map generator, produce map with <em>lots</em> of different armies and then test and rebalance it until it was ready to be used as a training map. It was quite the effort and ultimately resulted in an entire system comprised of several components:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-mapgen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System for generating train (and test) data </div> <p>The cycle goes like this:</p> <ol> <li>Generate a map with many heroes (i.e. armies) of <em>roughly</em> similar strength</li> <li>Evaluate the map by collecting results for each army based on the battles lead by the built-in bot on both sides</li> <li>Rebalance the map by adding/removing units from the armies based on aggregated results from 2.</li> <li>Repeat step 2. and 3. until no further rebalancing is needed</li> </ol> <p>Speaking in numbers, I have been using maps with 4096 heroes, which means there are more than 16M unique parings, making it very unlikely that the agent will ever train with the exact same army compositions twice. There‚Äôs a catch here: in HOMM3, there‚Äôs no way to have 4096 heroes on the map at the same time (the hero limit is 8). Fortunately, VCMI has no such limit, but imposes another restriction: there can‚Äôt be duplicate heroes on the map, and the base game features less than 200 heroes in total. To make this possible, I had to create a VCMI ‚Äúmod‚Äù which adds each of those 4096 heroes as a separate hero type and the issue was gone:</p> <p>That partly solved the issue with the diverse training data. The only issue with this approach was that all armies were of equal strength - for example, all armies were corresponding to a mid-game army (60-90 in-game days). This meant that the agent had less training experience with early-game (weaker) armies, as well as late-game (stronger) armies. I later improved this map generator and added the option to create <code class="language-plaintext highlighter-rouge">pools</code> of heroes, where all heroes within a pool have equal strength, but heroes in different pools have different stregths. Here is how one such map looks like:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-testmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Procedurally generated map with 4096 unique hero armies organized into 4 pools. Here, a hero with ID=85 that belongs to a pool of armies with total <a href="https://heroes.thelazy.net/index.php/AI_value" rel="external nofollow noopener" target="_blank">AI Value</a> of 10K (early-game army pool) is selected. </div> <p>By training on those maps, agents were able to generalize well and maintain steady performance across a wide variety of situations. Below are the results achieved by an agent trained on a 4096-army map and then evaluated on an unseen data set:</p> <div class="row justify-content-md-center"> <div class="col-sm-11"> <figure> <picture> <img src="/assets/img/vcmi-gym/rl-mapgen-results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Agent evaluation results from simulated early-game (20K) and late-game (300K) battles </div> <p>The dataset was unseen in the sense that it included armies of total strength 20K and 300K (neither can be found in the training dataset). The battle terrain and obstacles were not exactly unseen, as there is a finite combination of possible terrains and the agent had probably encountered them all before. The enemy behaviour, however, was also new, since all training is done vs. the scripted ‚ÄúStupidAI‚Äù bot, while half of the evaluation is done vs. ‚ÄúBattleAI‚Äù (VCMI‚Äôs strongest scripted bot).</p> <p>Having said all this, I still haven‚Äôt described how exactly I evaluate the those models, so keep reading ahead if you want to know more.</p> <h5 id="Ô∏è-testing-evaluation">üïµÔ∏è Testing (evaluation)</h5> <p>Evaluating the trained agents is a convenient way to determine if the training is productive. A typical problem that occurs during training is overfitting: the metrics collected during training are good, but the metrics collected during testing are not.</p> <p>At the time I was training my models on 3 different machines at home (poor man‚Äôs RL training cluster), each of which was exporting a new version of the model being trained at regular intervals. I used W&amp;B as a centralized storage (they give you 100GB of storage on the free tier, which is amazing) for models and and designed an evaluator which could work in a distributed manner (i.e. in parallel with other evaluators on different machines), pulling stored models, evaluating their performance and pushing the results. Here‚Äôs how it looks:</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/components-rl-evaluators.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System for evaluating trained models </div> <p>At regular intervals, the hyperparam optimizer uploads W&amp;B artifacts containing trained models. The evaluators download those models, load them and let them play against multiple types of opponents (StupidAI and BattleAI - the two in-game scripted bots) on several different maps which contain unseen army compositions. The results of those battles are stored for visualisation and the models - marked as evaluated (using W&amp;B artifact tags) to prevent double evaluation.</p> <h5 id="-neural-network">üß† Neural Network</h5> <p>Along with the reward scaling, the thing I experimented the most with was the architectures of the neural network. Various mixtures of fully-connected (FC), convolutional (Conv1d, Conv2d), residual (ResNet), recurrent (LSTM) and self-attention layers were used in this project with varying success. In general, results for NNs which contained SelfAttention and/or LSTM layers were simply worse, so I eventually stopped experimenting with them. Similarly, batch normalization and dropout also seemed to do more harm than good, so those were removed as well.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-fc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-heads.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-v4-lstm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-v4-deep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Some of the NN architectures used in vcmi-gym (there are way too many variations to visualize them all) </div> <p>.</p> <p>.</p> <p>.</p> <h2 id="fast-forward-to-2025">Fast forward to 2025</h2> <p>It‚Äôs been a year now since I‚Äôve last published any updates here. In short, I started a new job at GATE (Big Data for Smart Society Institute) and, most importantly, my son Stefan was born!</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/stefan.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Meet <b>Stefan</b>: my new reinforcement learning agent in real life. </div> <p>Time became a scarce resource. I remained committed to the vcmi-gym/MMAI project and managed to sneak a couple of hours nearly every day, but I did not have the bandwidth to turn that work into coherent updates. In an attempt to catch up, I will summarize what I tried, what failed, what worked, and what finally made MMAI feel like a serious contribution rather than a prototype.</p> <h2 id="the-first-mmai-contribution">The first MMAI contribution</h2> <p>In October 2024, I submitted MMAI in a <a href="https://github.com/vcmi/vcmi/pull/4788" rel="external nofollow noopener" target="_blank">pull request</a> to the VCMI repo.</p> <p>It contained a single model. At that point I had already observed that training two separate policies (one for attacker and one for defender) consistently outperformed a single ‚Äúuniversal‚Äù policy. Since neutral fights in VCMI always place neutrals on the defender side, the model I shipped was <strong>defender-only</strong>, and the initial integration allowed choosing MMAI only as the <strong>neutral AI</strong>.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/screenshot-vcmi-launcher-2024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> MMAI selectable as a NeutralAI in the VCMI launcher </div> <p>Key characteristics of the initial submission model:</p> <ul> <li> <strong>Architecture:</strong> a small variation of the neural network described earlier.</li> <li> <strong>Performance:</strong> <ul> <li>~75% win rate vs. StupidAI (VCMI‚Äôs ‚Äúweak‚Äù scripted bot)</li> <li>~45% win rate vs. BattleAI (VCMI‚Äôs ‚Äústrong‚Äù scripted bot)</li> </ul> </li> </ul> <p>The contribution did not exactly go as planned: there were mixed reactions and it became apparent that there‚Äôs more to be desired if MMAI were to become a part of VCMI.</p> <h4 id="why-the-pr-stalled">Why the PR stalled</h4> <p>The community feedback was direct, and it was fair. Some of the main issues were:</p> <ol> <li> <strong>Strength:</strong> the model was still weaker than BattleAI, so the practical value of merging it was questionable.</li> <li> <strong>Play experience:</strong> it was not enjoyable to play against. It behaved too passively ‚Äì often backing away and waiting, effectively forcing the human player to engage first.</li> <li> <strong>Creature bank battlefields:</strong> it struggled with the circular/irregular troop placement used in creature banks. In particular, it behaved poorly for stacks in the top-left corner because it had almost never been trained on those layouts. The result looked like a bug, even if the underlying cause was distribution shift.</li> </ol> <p>That seemingly took the steam out of this contribution, so instead of pushing for the merge, I took a step back to see what can be improved.</p> <h2 id="back-to-the-drawing-board">Back to the drawing board</h2> <p>I iterated across several axes at once:</p> <ul> <li>different NN architectures</li> <li>different RL algorithms</li> <li>different observation and action spaces</li> <li>different reward functions</li> </ul> <p>Some of these paths ended up being expensive lessons.</p> <p>Iterations took days, sometimes weeks, and experiments were effectively sequential. I had to step-up my game: I moved from local hardware to rented GPUs on VastAI, and migrated long-term storage to AWS S3. That improved iteration speed dramatically.</p> <p>I also reworked the sampling/training pipeline: instead of many independent PBT workers each running their own small workload, I leaned into gymnasium vector environments (with a patch to make VCMI play nicely) so that inference and learning could run in larger batches and the GPU would stop idling. That enabled me to more freely carry out the experminets that followed.</p> <hr> <h3 id="dreamerv3">DreamerV3</h3> <p><em>Significant effort, zero learning.</em></p> <p>I spent a substatial amount of time trying to make DreamerV3 work, using:</p> <ul> <li>a modified ray[tune] setup, and</li> <li>SheepRL‚Äôs implementation adapted for <strong>masked action spaces</strong>.</li> </ul> <p>Despite eventually getting the training pipeline running, the models simply did not learn. Not ‚Äúlearned slowly‚Äù - they failed to learn even basic competence across the hyperparameter space I tried. I never found the root cause, and at some point continuing to push here stopped being rational. I dropped DreamerV3 and decided to move on. At least I had managed to submit several Ray <a href="https://github.com/ray-project/ray/pulls?q=is%3Apr+author%3Asmanolloff+is%3Aclosed" rel="external nofollow noopener" target="_blank">bug fixes</a> along the way.</p> <hr> <h3 id="mcts-alphazeromuzero-family">MCTS (AlphaZero/MuZero family)</h3> <p><em>Attractive in theory, impractical in VCMI.</em></p> <p>For turn-based, adversarial, positional games, MCTS-style approaches (AlphaZero, MuZero, etc.) are a natural temptation. The problem is structural: to do tree search, you need an environment that can <strong>branch</strong>:</p> <ul> <li>step forward N actions,</li> <li>roll back,</li> <li>step forward N different actions,</li> <li>roll back again,</li> <li>repeat.</li> </ul> <p>VCMI battles are not designed to ‚Äúrewind‚Äù cleanly. Rolling back the last N actions reliably is a non-trivial engineering project on its own. I noticed there‚Äôs a fellow AI enthusiast on the VCMI <a href="https://discord.com/channels/298106089885401090/1147259775420207256" rel="external nofollow noopener" target="_blank">discord channel</a> who is working on an MCTS-based AI model, hopefully he will be able to overcome this limitation.</p> <p>Instead, I decided to explore the alternative: train a model that can <em>simulate</em> battle progression.</p> <hr> <h3 id="simulation-models">Simulation models</h3> <p>Simulating sequences of turns in VCMI battles requires two capabilities:</p> <ol> <li> <strong>State transitions</strong> (what the world becomes after an action)</li> <li> <strong>Opponent behavior</strong> (what the enemy does in a given state)</li> </ol> <p>Those are distinct problems, so I trained two models:</p> <ol> <li> <strong>Transition model</strong> (codename <code class="language-plaintext highlighter-rouge">t10n</code>): <ul> <li>input: (battlefield_state, chosen_action)</li> <li>output: predicted_next_state</li> </ul> </li> <li> <strong>Policy/prediction model</strong> (codename <code class="language-plaintext highlighter-rouge">p10n</code>): <ul> <li>input: battlefield_state</li> <li>output: predicted_action (e.g. what BattleAI would do)</li> </ul> </li> </ol> <h4 id="how-one-imagined-timestep-works">How one imagined ‚Äútimestep‚Äù works</h4> <p>Assume it is <em>our</em> turn as the model, playing as defender. We want to evaluate action X without executing it in VCMI.</p> <p>Pseudo-code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state</span> <span class="o">=</span> <span class="n">obs</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">candidate_action</span>

<span class="c1"># roll forward until the turn comes back to us
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="nf">t10n</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">next_state</span><span class="p">.</span><span class="n">side_to_act</span> <span class="o">==</span> <span class="sh">"</span><span class="s">us</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">p10n</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>  <span class="c1"># predict opponent response state = next_state
</span>
<span class="k">return</span> <span class="n">next_state</span>
</code></pre></div></div> <p>Once a single timestep can be imagined like this, imagining a horizon-H trajectory is just a matter of repeating the above loop H times.</p> <h4 id="training-the-simulation-models">Training the simulation models</h4> <p>Unlike online RL, training <code class="language-plaintext highlighter-rouge">t10n</code> and <code class="language-plaintext highlighter-rouge">p10n</code> is essentially supervised learning on logged transitions:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">t10n</code> training pairs: (state, action) ‚Üí next_state</li> <li> <code class="language-plaintext highlighter-rouge">p10n</code> training pairs: state ‚Üí action</li> </ul> <p>That decoupling is convenient because it allows offline data collection, but it comes at a cost: data volume.</p> <p>A single sample (two observations + one action) was ~100KB. At that size:</p> <ul> <li>1M samples ‚âà 100GB</li> <li>‚Äúmillions‚Äù of samples quickly becomes <strong>terabytes</strong> </li> </ul> <p>Two immediate bottlenecks followed:</p> <ol> <li> <strong>Storage cost:</strong> not only S3, but also VM storage (VastAI charges you per GB of storage used).</li> <li> <strong>Transfer time:</strong> repeatedly downloading terabytes of data is slow and costly (VastAI charges per GB downloaded, as well as a fixed charge for renting the VM)</li> </ol> <p>Fortunately, NumPy‚Äôs built-in compressed storage reduced sample size by roughly <strong>10√ó‚Äì20√ó</strong>. That made the dataset manageable, at the price of CPU overhead for (de)compression‚Äîwhich was acceptable compared to the storage and transfer costs.</p> <p>With vectorized environments, PyTorch data loaders, and a custom packing format, I eventually collected millions of samples and had enough data to train both models.</p> <h4 id="loss-functions">Loss functions</h4> <p><em>Easy for <code class="language-plaintext highlighter-rouge">p10n</code>, non-trivial for <code class="language-plaintext highlighter-rouge">t10n</code>.</em></p> <p>For <code class="language-plaintext highlighter-rouge">p10n</code>, the output is an action distribution, and the target is a one-hot action. Standard cross-entropy works well.</p> <p>For <code class="language-plaintext highlighter-rouge">t10n</code>, the state contains a mixture of feature types:</p> <ol> <li> <strong>Continuous</strong> (e.g. HP normalized to [0,1])</li> <li> <strong>Binary</strong> (e.g. traits/flags)</li> <li> <strong>Categorical</strong> (e.g. slot IDs, unit types, etc.)</li> </ol> <p>A single MSE loss is a poor fit. Instead I used a composite objective:</p> <ul> <li>MSE for continuous</li> <li>BCE for binary</li> <li>CE for categorical</li> </ul> <p>Conceptually:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">L</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">continuous</span><span class="p">)</span> <span class="o">+</span> <span class="nf">bce</span><span class="p">(</span><span class="n">binary</span><span class="p">)</span> <span class="o">+</span> <span class="nf">ce</span><span class="p">(</span><span class="n">categorical</span><span class="p">)</span>
</code></pre></div></div> <p>After a couple of months of training and iteration I had a <code class="language-plaintext highlighter-rouge">t10n</code> and <code class="language-plaintext highlighter-rouge">p10n</code> pair that looked promising.</p> <h4 id="handling-simulation-uncertainty">Handling simulation uncertainty</h4> <p>The first time I tried to render <code class="language-plaintext highlighter-rouge">t10n</code>‚Äôs predicted output, it became obvious that post-processing was required.</p> <p><code class="language-plaintext highlighter-rouge">t10n</code> outputs <em>probabilities</em>, not discrete game states. For binary values, that means outputs like <code class="language-plaintext highlighter-rouge">0.93</code> instead of <code class="language-plaintext highlighter-rouge">1</code>. This is not necessarily wrong: some transitions are genuinely stochastic (e.g. paralysis chance). The model reflects that uncertainty.</p> <p>To <em>render</em> a state, however, VCMI needs discrete values, i.e. you can‚Äôt render a ‚Äú70%‚Äù paralyzed creature - it must be either paralyzed or not. So, the predicted state has to be <strong>collapsed</strong> into a concrete instance:</p> <ul> <li>binary flags snapped to 0/1</li> <li>categorical logits turned into an argmax category</li> <li>constraints enforced so that invalid combinations do not survive</li> </ul> <p>With this, a probabalistic state could be materialized into concrete instances.</p> <hr> <h3 id="the-world-model">The ‚Äúworld model‚Äù</h3> <p><em>Usable, but not stable enough for serious MCTS.</em></p> <p>Combining <code class="language-plaintext highlighter-rouge">t10n</code> and <code class="language-plaintext highlighter-rouge">p10n</code> gave me a ‚Äúworld model‚Äù capable of imagining future rollouts.</p> <p>A <em>rollout</em> is a sequence of <em>timesteps</em>, which in turn are sequences of <em>transitions</em>.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/diagram-timestep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Rollouts, timesteps and transitions. </div> <p>The world model is capable of simulating only <em>transitions</em>, hence timesteps and rollouts must be generated in an autoregressive manner. This naturally leads to a fundamental problem: <strong>autoregressive error accumulation</strong>. It manifests as innacuracies and hallucinations, resulting in invalid states. For example, a unit which <em>may have been paralyzed</em> would mean its ‚Äúparalyze‚Äù flag (which should be either 1 or 0) is somewhere in-between, e.g. <code class="language-plaintext highlighter-rouge">0.4</code>. There is no such thing as a ‚Äú40% paralyzed creature‚Äù in VCMI - it‚Äôs either paralyzed or not.</p> <p>To contain this, I collapsed states after every transition and fed the collapsed versions forward. You can think of collapsing as rounding in the case of numeric is a <em>round</em> operation. For categoricals, this That eliminated many reconstruction failures, but it exposed the model‚Äôs hallucinations more clearly:</p> <ul> <li>some long exchanges caused excessive drift</li> <li>units sometimes morphed into ‚Äúhybrids‚Äù (e.g. a First Aid Tent drifting toward a Dendroid Guard-like unit)</li> <li>end-of-battle uncertainty was particularly destructive (once the model becomes unsure the battle ended, it becomes unsure about everything after)</li> </ul> <p>This is best illustrated by comparing the model‚Äôs imagined transition sequence (I call it the <em>dream</em>) against the actual transitions occurring in the environment:</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/world-model-real.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/world-model-dream.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparing transition sequences: VCMI's <b>actual</b> sequence (top) vs. my world-model's <b>dream</b> (bottom). </div> <p>Note: Blue units <code class="language-plaintext highlighter-rouge">2</code> (Pegasus), <code class="language-plaintext highlighter-rouge">4</code> (Pegasus) and <code class="language-plaintext highlighter-rouge">5</code> (Death Knight) are <em>wide</em> units and occupy two hexes, marked as <code class="language-plaintext highlighter-rouge">2‚Üê2</code>, <code class="language-plaintext highlighter-rouge">4‚Üê4</code> and <code class="language-plaintext highlighter-rouge">5‚Üê5</code>.</p> <p>In this test, we play as red, while VCMI‚Äôs BattleAI bot plays as blue.</p> <p>The actual in-game transitions are as follows:</p> <ol> <li> <strong>Initial condition:</strong> <ul> <li>red unit <strong>0</strong> is active</li> <li>the next action is: move to y=8 x=5.</li> </ul> </li> <li> <strong>First transition:</strong> <ul> <li>blue unit <strong>2</strong> is active</li> <li>the next action is: attack-move to hex y=7 x=4, striking at red unit <strong>0</strong>.</li> </ul> </li> <li> <strong>Second transition:</strong> <ul> <li>blue unit <strong>2</strong> is active <ins>again</ins> (perhaps it has waited earlier)</li> <li>the next action is: attack-move to hex y=7 x=4, striking at red unit <strong>0</strong> (again)</li> </ul> </li> <li> <strong>Third transition:</strong> <ul> <li>red unit <strong>0</strong> was killed.</li> <li>battle has ended.</li> </ul> </li> </ol> <p>Comparing this against the world model‚Äôs dream (imagined transitions):</p> <ol> <li> <strong>Initial condition:</strong> same as above.</li> <li> <strong>First <em>imagined</em> transition:</strong> also same as above. The world model has correctly simulated this transition by simulating a new state given the previous state + action, as well as predicting the enemy‚Äôs next action.</li> <li> <strong>Second <em>imagined</em> transition:</strong> a distorted version of the actual state: <ul> <li>a <em>phantom</em> blue unit is active (it‚Äôs not visible on the map).</li> <li>the predicted action is: attack-move to y=7 x=6, striking at red unit <strong>0</strong>. This is no longer the same as the actual transition, but it‚Äôs close (apparently, the model becomes uncertain when the same unit has to act twice).</li> </ul> </li> <li> <strong>Third <em>imagined</em> transition:</strong> an even blurrier version of the actual state: <ul> <li>the <em>phantom</em> unit has materialized as blue unit <strong>0‚Üê</strong>. The arrow indicates this is a wide unit (occupying 2 hexes), but its second hex is marked ‚Äúunreachable‚Äù (dark circle).</li> <li>red unit <strong>0</strong> was <em>not</em> killed.</li> <li>the battle has <em>not</em> ended.</li> <li>blue unit <strong>4</strong> is active.</li> </ul> </li> </ol> <p>In this dream, the battle continues. The imagined states have become disconnected from the actual ones. Looking at the creatures‚Äô stats, we can see a numerical drift as well:</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/world-model-real-final.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/world-model-dream-final.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Unit stats after the 3 transitions: <b>actual</b> stats (left) vs. the stats in the <b>dream</b> (right). </div> <p>Comparing the actual vs imagined stats:</p> <ul> <li>There is a noticeable drift, although some similarity remains. Looking carefully, you will notice that creatures in the dream appear <em>more powerful</em> than they actually are - an interesting topic on its own.</li> <li>The <em>phantom</em> unit (blue unit <strong>0</strong>) is very similar to blue unit <strong>2</strong>. As if unit <strong>2</strong> was <em>split in two</em>, preserving the overall strenth of the blue army and resulting in one extra unit.</li> <li>The <em>still alive</em> red unit <strong>1</strong> has barely survived and will <em>not</em> act for another 17 turns (Queue=18). This is a signals that the model is uncertain if the unit is <em>dead or alive</em>.</li> </ul> <p>This shows how the imagined states deeper in the dream drift further away from reality. Not surprising, but I find it oddly satisfying to observe. Unfortunately, such a model is not a viable replacement for the VCMI game engine for the purposes of MCTS, which requires a much more faithful simulator. With no mature open-source MuZero-like stack I could practically adapt, the expected return on further investment dropped sharply.</p> <p>I did try <code class="language-plaintext highlighter-rouge">muax</code> because it is referenced from DeepMind‚Äôs mctx ecosystem, but it turned out to be a thin wrapper around assumptions that did not fit my use case (no batching/vectorization, no support for GPU computation, strong constraints on observation/action types, etc.). The timing was unfortunate ‚Äî I found these constraints late, but the detour was still useful: I learned to write JAX by rewriting my models in JAX/Flax, then rewriting them again for Haiku. It was an instructive exercise and I would definitely keep Jax in mind for my future projects, but it did not unlock MCTS.</p> <p>At that point, I pivoted to a different imagination-based idea: I2A.</p> <hr> <h3 id="the-i2a-model">The I2A model</h3> <p>I2A (Imagination-Augmented Agents), introduced in a research paper from 2017, integrates a learned world model into an otherwise model-free policy by adding an ‚Äúimagination core‚Äù that generates rollouts, which the policy learns to interpret. The key claim that caught my attention: I2A can still be useful even when the environment model is imperfect.</p> <div class="row justify-content-md-center"> <div class="col-sm-8"> <figure> <picture> <img src="/assets/img/vcmi-gym/i2a-architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> I2A architecture diagram (source: <a href="https://arxiv.org/pdf/1707.06203" rel="external nofollow noopener" target="_blank">I2A paper</a>) </div> <p>My <code class="language-plaintext highlighter-rouge">t10n + p10n</code> world model looked like a fit. Unfortunately, VCMI‚Äôs action space makes this approach computationally brutal.</p> <p>With:</p> <ul> <li>horizon = 5 timesteps</li> <li>trajectories = 10</li> <li>batch_size = 10 steps</li> </ul> <p>‚Ä¶the number of forward passes exploded exponentially (including a third reward model, omitted here for brevity). The overhead was so large that learning slowed to a crawl. Even with aggressive pruning (10 trajectories is well under 30% of the actions a unit can take on average), the compute budget was dominated by ‚Äúimagination‚Äù, not learning.</p> <p>I2A was not viable for this problem under these constraints.</p> <hr> <h3 id="mppo-dna-model-revised">MPPO-DNA model (revised)</h3> <p><em>The action space is the enemy.</em></p> <p>At this point I returned to MPPO-DNA and focused on what repeatedly hurt almost every algorithm I tried:</p> <p><strong>2312 discrete actions</strong>.</p> <p>I had previously attempted a multi-head policy that decomposes the action into several parts (inspired by ‚Äúmini-AlphaStar‚Äù-style spatial/non-spatial factorization), but that attempt did not converge well. This time I restarted the implementation from scratch to avoid ‚Äúfixing‚Äù myself into the same design.</p> <h4 id="hex-aware-spatial-processing">Hex-aware spatial processing</h4> <p>A major architectural change was implementing a custom 2D convolution-like layer for a <strong>hex grid</strong>. Standard 2D convolutions assume a square lattice; VCMI battlefields are hex-based. Treating hexes as squares is possible, but it causes geometric distortion.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/conv-classic-square.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/conv-classic-hex.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/conv-hexconv-square.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/conv-hexconv-hex.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Classic convolutional kernels (top) cause a geometric distortion when applied to hex grids. <br> I designed a hex-convolutional kernel (bottom) to address this issue. </div> <h4 id="adding-transformers">Adding transformers</h4> <p>Incorporating Transformer encoder layers attending over the hexes was the first time I saw models reach <strong>~50% win rate vs BattleAI</strong>. The layers added were:</p> <ul> <li>self-attention (hex ‚Üî hex)</li> <li>cross-attention (global state ‚Üí hexes)</li> </ul> <p>That was a genuine milestone, but there was a catch: models became unstable.</p> <h4 id="the-slump">The slump</h4> <p>Transformer-based models often trained well, but then abruptly collapsed. Lower learning rates, gradient norm clipping and reducing the number of transformer layers were among the first things I tried, but none worked. Collapsing persisted across runs. I still do not have a reasonable explanation.</p> <p>Still, the direction felt correct: I wanted global context propagation without destroying spatial structure. That led naturally to the next step.</p> <hr> <h3 id="the-gnn-pivot">The GNN pivot</h3> <p><em>Everything can be represented as a graph.</em></p> <p>Graph neural networks (GNN) are specialized artificial neural networks that are designed for tasks whose inputs are graphs. GNNs have enjoyed great recognition in the recent years, with DeepMind‚Äôs <a href="https://deepmind.google/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/" rel="external nofollow noopener" target="_blank">GraphCast</a> (deemed ‚Äúthe most accurate 10-day global weather forecasting system in the world‚Äù) and <a href="https://en.wikipedia.org/wiki/AlphaFold" rel="external nofollow noopener" target="_blank">AlphaFold</a> (a 2024 Nobel Price winning protein folding predictor) being notable examples. I decided to explore a GNN-based approach for vcmi-gym, which meant I had to first and foremost find out how to represent the VCMI battlefield as a graph.</p> <p>Graphs are an amazing way to model information - they excel at modeling objects (called graph nodes), their relationships (called graph edges). Feeding this into a graph neural network (GNN), the processing of the information flow through those edges (called message passing) can be further adapted for efficient learning.</p> <p>For VCMI I defined a <a href="https://pytorch-geometric.readthedocs.io/en/2.6.0/notes/heterogeneous.html" rel="external nofollow noopener" target="_blank">heterogenous graph</a> with a single node type (<code class="language-plaintext highlighter-rouge">HEX</code>) and seven edge types. The graph contains a fixed number of nodes (165) and a varying number of edges (depending on the unit composition, position, abilities, etc.):</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-actsbefore.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-adjacent.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-reach.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-rangeddmg.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-meleedmg.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/edge-retaldmg.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An attempt to visualise the graph topology. Nodes are represented by circles, edges are represented by arrows. <br> For readability's sake, only the <b>outgoing</b> edges for exactly <b>one</b> node are drawn (in the real graph, <i>each</i> node has similar outgoing edges). </div> <p>This representation matched how I reason about the battlefield when playing the game: not as a flat tensor, but as entities and relationships, in particular:</p> <ul> <li><em>How much damage can unit X deal to unit Y?</em></li> <li><em>Can unit X reach hex Y?</em></li> <li><em>Will unit X act before unit Y?</em></li> <li>etc.</li> </ul> <p>All this required some C++ work and a new MMAI observation version: <strong>v12</strong> (yes, by that time, I had reached the <em>12th</em> iteration), but it worked well. Next up was the neural network design, which governs how this information is processed.</p> <p>Different GNNs have different message passing logic. Finding a suitable message passing function is not trivial, and sometimes a customised approach is needed. <a href="https://pytorch-geometric.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">PyTorch Geometric</a> already contains implementations for many of the popular GNNs, so I ran a small suite of experiments across several of them in parallel to see which one would be most promising:</p> <ul> <li>GCNConv</li> <li>GATv2Conv</li> <li>TransformerConv</li> <li>GINEConv</li> <li>PNAConv</li> <li>GENConv</li> </ul> <p>I also experimented with various graph design choices along the way:</p> <ul> <li>static vs dynamic nodes</li> <li>directed vs undirected vs bidirectional edges</li> <li>homogeneous vs heterogeneous graphs</li> <li>different message passing and attention scoring schemes</li> </ul> <p>Some of those experiments delivered promising results. I was on the right track.</p> <h4 id="the-breakthrough">The breakthrough</h4> <p><em>At long last, MMAI supremacy!</em></p> <p>Among the configurations I tried, the <strong>GENConv-based</strong> models trained on a dynamic heterogeneous graph with directed edges and attention-style scoring seemed to perform best.</p> <p>Further experiments based on similar configurations rewarded me with exceptional results: win rates vs. BattleAI climbed to an average of <strong>65%</strong>. After a year of ‚Äúalmost there‚Äù, this finally felt <em>good</em>.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/gnn-chart-v12-winrate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The 12th iteration of MMAI: win rates during vs. VCMI's strongest "BattleAI" bot. This chart tracks the model's performance during the 5 full days of training. </div> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/arch-gnn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> MMAI v12: NN architecture diargram (SVG version <a href="/assets/img/vcmi-gym/arch-gnn.svg" target="_blank">here</a>) </div> <p>This was it. Now I wanted to try and play against this model, 1v1, me vs. MMAI! To do it, I first had to find a way to <em>export</em> it from vcmi-gym and then load it in the standalone game.</p> <hr> <h3 id="exporting-the-model">Exporting the model</h3> <p>The charts were looking great, but it was time to test it out myself. However, it was still a PyTorch Geometric model i.e. just a Python artifact. To load it inside VCMI, I needed a C++ loadable format. TorchScript exports had previously been my path, but PyTorch‚Äôs model export for edge devices such as mobile phones has been deprecated in favor of <a href="https://docs.pytorch.org/executorch/stable/index.html" rel="external nofollow noopener" target="_blank">executorch</a>, so I went for it instead.</p> <h4 id="executorch-xnnpack">ExecuTorch: XNNPACK</h4> <p>ExecuTorch supports multiple backends (CoreML, Vulkan, XNNPACK, OpenVINO, etc.). The difficulty is that VCMI targets a wide platform matrix (Windows/Linux/macOS/iOS/Android; multiple architectures), while most backends are platform-specific.</p> <p>XNNPACK was the only backend that looked plausibly universal. In practice, making the model lowerable to XNNPACK required a heavy redesign due to the limitations imposed by the limited available opset:</p> <ul> <li>no dynamic shapes</li> <li>no ‚Äúsmart‚Äù indexing</li> <li>softmax/argmax/sampling-style logic had to rewritten with primitive tensor ops (gather/scatter/index_select style)</li> <li>PyG structures (HeteroData/HeteroBatch) had to be replaced by flat, fixed-shape tensors</li> <li> <code class="language-plaintext highlighter-rouge">inf</code> masks had to be replaced with large finite negatives</li> </ul> <p>In addition, the fixed-size limitation meant that dynamic graphs were not an option, so support for several different fixed sizes (<strong>buckets</strong>) was added to avoid wasting compute (the appropriate bucket is chosen at runtime based on the graph size).</p> <p>Bucket definitions were guided by size statistics collected over ~10,000 observations:</p> <table> <thead> <tr> <th></th> <th colspan="7" class="text-center">Number of edges <code>E</code> (total)</th> </tr> <tr> <th>Edge type</th> <th>avg</th> <th>max</th> <th>p99</th> <th>p90</th> <th>p75</th> <th>p50</th> <th>p25</th> </tr> </thead> <tbody> <tr> <td><code>ADJACENT</code></td> <td>888</td> <td>888</td> <td>888</td> <td>888</td> <td>888</td> <td>888</td> <td>888</td> </tr> <tr> <td><code>REACH</code></td> <td>355</td> <td>988</td> <td>820</td> <td>614</td> <td>478</td> <td>329</td> <td>209</td> </tr> <tr> <td><code>RANGED_MOD</code></td> <td>408</td> <td>2403</td> <td>1285</td> <td>646</td> <td>483</td> <td>322</td> <td>162</td> </tr> <tr> <td><code>ACTS_BEFORE</code></td> <td>51</td> <td>268</td> <td>203</td> <td>118</td> <td>75</td> <td>35</td> <td>15</td> </tr> <tr> <td><code>MELEE_DMG_REL</code></td> <td>43</td> <td>198</td> <td>160</td> <td>103</td> <td>60</td> <td>31</td> <td>14</td> </tr> <tr> <td><code>RETAL_DMG_REL</code></td> <td>27</td> <td>165</td> <td>113</td> <td>67</td> <td>38</td> <td>18</td> <td>8</td> </tr> <tr> <td><code>RANGED_DMG_REL</code></td> <td>12</td> <td>133</td> <td>60</td> <td>29</td> <td>18</td> <td>9</td> <td>4</td> </tr> </tbody> </table> <p><br></p> <table> <thead> <tr> <th></th> <th colspan="7" class="text-center">Number of inbound edges <code>K</code> (per hex)</th> </tr> <tr> <th>Edge type</th> <th>avg</th> <th>max</th> <th>p99</th> <th>p90</th> <th>p75</th> <th>p50</th> <th>p25</th> </tr> </thead> <tbody> <tr> <td><code>ADJACENT</code></td> <td>5.4</td> <td>6</td> <td>6</td> <td>6</td> <td>6</td> <td>6</td> <td>6</td> </tr> <tr> <td><code>REACH</code></td> <td>2.2</td> <td>13</td> <td>10</td> <td>8</td> <td>6</td> <td>4</td> <td>3</td> </tr> <tr> <td><code>RANGED_MOD</code></td> <td>2.5</td> <td>15</td> <td>8</td> <td>4</td> <td>3</td> <td>2</td> <td>1</td> </tr> <tr> <td><code>ACTS_BEFORE</code></td> <td>0.3</td> <td>23</td> <td>19</td> <td>15</td> <td>12</td> <td>8</td> <td>5</td> </tr> <tr> <td><code>MELEE_DMG_REL</code></td> <td>0.3</td> <td>10</td> <td>9</td> <td>8</td> <td>7</td> <td>5</td> <td>3</td> </tr> <tr> <td><code>RETAL_DMG_REL</code></td> <td>0.2</td> <td>10</td> <td>9</td> <td>8</td> <td>6</td> <td>5</td> <td>3</td> </tr> <tr> <td><code>RANGED_DMG_REL</code></td> <td>0.1</td> <td>8</td> <td>6</td> <td>3</td> <td>2</td> <td>2</td> <td>1</td> </tr> </tbody> </table> <p><br></p> <p>This took a long time, but at least I gained a solid understanding of the GNN implementation internals while working on the solution.</p> <p>Executorch did work but performance was not good enough ‚Äì below are the measured time for a single action prediction on a small battlefield (bucket ‚ÄúS‚Äù):</p> <p>700ms is not acceptable in real gameplay: imagine a single-player game with 6 computer players, each of which fights one or more battles during their turn (each battle has involves at least 10 predictions) - that‚Äôs 30+ seconds, and if you‚Äôve ever played HOMM3, you know that computer turns should be much faster than that (usually less than 10 seconds in total).</p> <p>Additionally, I ran into platform/toolchain issues and memory violation errors on Windows. I spend considerable amount of time trying to fix it, but eventually gave up. I needed a different deployment strategy.</p> <h4 id="executorch-vulkan-and-coreml">ExecuTorch: Vulkan and CoreML</h4> <p>I explored Vulkan and CoreML exports to see if those would bring a viable performance improvement, but stumbled upon many problems:</p> <ul> <li>Vulkan had instability and platform build issues and the C++ code ultimately failed to load models on android devices due to a cryptic shader-related error.</li> <li>CoreML displayed good runtime performance on Apple devices, but the CoreML models were several times larger in size and the very first forward pass of those models took a whopping 10 seconds on iOS which felt pretty bad.</li> </ul> <p>Neither was a good solution, there had to be a better way.</p> <h4 id="libtorch-revised">Libtorch (revised)</h4> <p>I returned to libtorch. Compiling it from source is painful, so I <a href="[https://github.com/smanolloff/vcmi-libtorch-builds]">built it separately</a> and consumed it as an external library in VCMI.</p> <p>Libtorch had its drawbacks (such as large library size), but it was performing well and was more portable compared to ExecuTorch. I decided to go for it, and I updated the MMAI <a href="https://github.com/vcmi/vcmi/pull/4788" rel="external nofollow noopener" target="_blank">pull request</a> accordingly.</p> <h4 id="onnx">ONNX</h4> <p>During the PR review phase, <a href="https://github.com/Laserlicht" rel="external nofollow noopener" target="_blank">@Laserlicht</a> suggested I should replace libtorch with <a href="https://onnxruntime.ai" rel="external nofollow noopener" target="_blank">ONNX Runtime</a> because it has solid packaging support and broad platform coverage. Having spent so much effort to replace libtorch with ExecuTorch, I was not exactly eager to try yet another alternative to libtorch, but the suggestion did seem reasonable, so it was worth a try.</p> <p>In a last-ditch effort to find a well-performaning solution with multi-platform support, I exported the MMAI models as ONNX graphs. The size footprint was small (as small as it could get with 11 million parameters, which is around 20MB). It was also relatively easy to wire it up in VCMI, but most notable, the inference benchmarks were surprising: ONNX models were <strong>~30% faster</strong> than their libtorch counterparts. It was enough for me, given it also provided overall smoother integration experience.</p> <p>With help from <a href="https://github.com/GeorgeK1ng" rel="external nofollow noopener" target="_blank">@GeorgeK1ng</a>, we also managed to produce working 32-bit Windows artifacts for the integration. This meant VCMI‚Äôs entire build matrix (a total of 16 different builds) is now buildable with MMAI support thanks to ONNX runtime!</p> <table> <thead> <tr> <th>Model Format</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <th>TorchScript</th> <td class="align-top"> <ul> <li>Rich OS support</li> <li>Rich NN op support</li> <li>Fast inference (40 / 120 / 240 ms)</li> </ul> </td> <td class="align-top"> <ul> <li>Deprecated</li> <li>No support for 32-bit platforms</li> <li>Unknown compatibility for edge devices</li> <li>Slow builds (&gt; 3h)</li> <li>Large size (20..70MB depending on platform)</li> </ul> </td> </tr> <tr> <th>ExecuTorch: XNNPACK</th> <td class="align-top"> <ul> <li>Rich OS support</li> <li>Fast builds (&lt; 15min)</li> <li>Small size (&lt; 5MB)</li> </ul> </td> <td class="align-top"> <ul> <li>Unusable on Windows</li> <li>Limited NN op support</li> <li>Slow inference (160 / 150 / 650 ms)</li> </ul> </td> </tr> <tr> <th>ExecuTorch: Vulkan</th> <td class="align-top"> </td> <td class="align-top"> <ul> <li>Android only</li> <li>Poor NN op support</li> <li>Could not make it work (error on load)</li> </ul> </td> </tr> <tr> <th>ExecuTorch: CoreML</th> <td class="align-top"> <ul> <li>Fast inference, but after warmup</li> </ul> </td> <td class="align-top"> <ul> <li>Apple only</li> <li>Slow warmup (1st forward pass takes 8s)</li> </ul> </td> </tr> <tr> <th>ONNX Runtime</th> <td class="align-top"> <ul> <li>Rich OS support</li> <li>Rich platform support</li> <li>Available <a href="https://conan.io/" rel="external nofollow noopener" target="_blank">Conan</a> recipe</li> <li>Fast inference (30 / 71 / 210 ms)</li> </ul> </td> <td class="align-top"> <ul> <li>ONNX model API exposes only a single (forward) method</li> </ul> </td> </tr> </tbody> </table> <p>NOTE: the inference values are results from my on 3 different devices: Mac M1 / iPhone 2020SE / Samsung Galaxy A71</p> <h4 id="playtesting">Playtesting</h4> <p>With libtorch set up, I could finally play the game vs my new model :) I quickly noticed a worrying sign:</p> <p>Even though the model played well, it had one persistent behavioral flaw: it was still too defensive ‚Äî often waiting for enemies to step within range, sometimes even running away, instead of proactively making attacks.</p> <p>This in particular was one of the early PR criticisms, so I treated it as a blocker and started looking for ways to fix it. I went with the simplest approach I could think of - a fixed, negative per-step reward - and it worked well: at the expense of a small drop in win rate vs BattleAI, I managed to greatly reduce the episode duration (i.e. number of turns in battle):</p> <div class="row"> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/gnn-chart-v13-len.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <img src="/assets/img/vcmi-gym/gnn-chart-v13-winrate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Compared to its predecessor, MMAI v13 manages similar win rates against BattleAI, but with a <b>significantly lower</b> turn count. Adjusting the reward function promoted a more aggressive behaviour without degrading performance, albeit at the cost of learning speed. </div> <p>Going on with the manual playthrough, I was pleased to find out that MMAI had learned a handful of important tactical moves, such as:</p> <ul> <li>actively trying to block shooters</li> <li>not attacking ‚Äúsleeping‚Äù units (paralyzed, petrified, etc.) to avoid waking them up</li> <li>repositioning blocked shooters instead of using their (weaker) melee attack</li> <li>staying out of enemy shooters‚Äô range, for the first few rounds</li> </ul> <p>On the other hand, on some occasions it was still making bad decisions, althoughsuch it occurred rarely now:</p> <ul> <li>may still play too defensively, waiting for the enemy to attack first, never taking the initiative</li> <li>may take ‚Äúbaits‚Äù, attacking cheap enemy units thrown forward, exposing its own</li> <li>may fail to recognize dead ends on battlefields where the randomly placed terrain obstacles form ‚Äúblind alleys‚Äù</li> </ul> <p>Nonetheless, my overall assessment was positive ‚Äì MMAI played reasonably well. Better than the bots, still not better than a human, but a worthy first-generation model for ML-powered AI in VCMI.</p> <hr> <h3 id="the-merge">The merge</h3> <p>In late 2025 (more than a year after the <a href="https://github.com/vcmi/vcmi/pull/4788" rel="external nofollow noopener" target="_blank">MMAI PR</a> was initially opened), I posted an update with my new MMAI v13 models. This sparked new discussions, suggestions, code reviews and improvements. The PR was merged into VCMI‚Äôs <code class="language-plaintext highlighter-rouge">develop</code> branch, earning a seat in the release train for the upcoming VCMI 1.7</p> <p>This means MMAI will finally see proper playtesting by human players. I have been collecting gameplay feedback from early testers in the VCMI community, and I expect much more to follow. Sadly, the feedback has been mostly negative so far :( Players seem to have rather high expectations, reporting the model‚Äôs bad decisions (such as the ones I outlined above) as issues to be fixed. Easier said than done, but they‚Äôre being honest and that‚Äôs what matters.</p> <p>My working theory is that the LLM boom in recent years has raised the AI bar to an extent where people take human-like AI behaviour as a baseline. LLMs and RL agents are fundamentally different and while transformers brought a revolution for all language models, we are yet to see a similar breaktrhough in reinforcement learning. Until then, comparing RL agents to LLMs is not a fair matchup. That being said, I know the criticism will still be useful for the upcoming generations of new MMAI models, and I already have a few ideas in mind for the next versions. Stay tuned :)</p> <hr> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Simeon Manolov. <br>Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?939a575388f286729c0279bcadb1d6f8"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>