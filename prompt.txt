i am writing a blog-style article where I describe my endeavour in training an RL agent to play VCMI (the open-source version of the game HOMM 3).
I have had a 1-year pause from updating that article since I started a new job at GATE institute https://www.gate-ai.eu/en/home/#lang-selector-mobile about a year ago, additionally my son was born recently, so overall I've had much less time to spare on the vcmi-gym project, although I remained committed to it and managed to sneak a couple of hours nearly every day since. Sadly, that left zero time for the article, that's why I maintained radio silence until now.
Now I want to update the article with info about what were my major struggles and achievements throughought the past year. Help me write it properly.
This is what I have so far:
https://smanolloff.github.io/projects/vcmi-gym/

and here is what happened in the past year:

* I submitted an initial version of MMAI https://github.com/vcmi/vcmi/pull/4788 with one model in October 2024. Since I had found out that training separate models for defender and attacker instead of 1 universal model gives better results, the model was trained as defender-only model and the initial version allowed to choose MMAI as the neutral AI only (since neutrals are always defenders).
* The initial version was based on a slight modification of the NN architecture mentioned above (the one on the last image)
* The model had a 75% winrate vs. StupidAI and 45% vs. BattleAI (VCMI's scripted "weak" and "strong" bots, respectively).

However, that model did not exactly enjoy the love I was hoping for, as there were a couple of issues with it that the VCMI community brought up:
- the model was not smart enough in the sense that it was still weaker than BattleAI, so introducing it made little sense
- the model was not fun to play against as it was too passive (running away from the enemy troops and essentially always waiting for the player to attack first)
- the model got confused in scenarios where the player was attacking a creature bank due to the unorthodox circular positions of the troops there - the model was not properly commanding troops at the top-left corner of the battlefield, because it was almost never trained on such battlefields. It was perceived as buggy.

Due to these issues, my initial contribution attempt stalled. So I went back to the drawing board and started working on new models with different NN architectures, RL algorithms, observation/action spaces and reward functions.

For example, I experimented with DreamerV3 via a modified version of ray[tune] and SheepRL's implementations by adapting the algorithm for masked action spaces, but I never managed to make the models learn anything with it, regardless which hyperparameter configurations I tried. I did submit a several [bug fixes](https://github.com/ray-project/ray/pulls?q=is%3Apr+author%3Asmanolloff+is%3Aclosed) to ray in the process. Nevertheless, after finally being able to actually start training models, I was once again hitting a wall, as the models failed to learn even the most basic tasks, regardless of which hyperparameter values I was using. I never managed to figure out the root cause, so I eventually abandoned DreamerV3 in favor of other RL algos.

For a while I had been eyeing Monte-Carlo Tree Search (MCTS) family of algorithms sich as AlphaZero, MuZero and friends, which are known to have had good success with positional, turn-based adversarial games such as chess. The problem there is that they need an environment which can go _back and forth_ in time for a configurable time-horizon (from a given timestep, perform N steps, go back, perform N different steps, go back, etc.) so they can perform their MCTS logic. With VCMI, this is too much to ask, as the game is not designed to easily "roll back" the last N in a battle. I had to train a separate model for this purpose.

Simulating sequences of steps and their outcomes naturally requires an environment model, which is able to mimic 1. state transitions and 2. enemy actions from a real environment. Those are pretty distinct responsibilities, so I decided to train two separate models for them:
1. A transition model (codename _t10n_): Given an battlefield state and an action to perform, predict the next battlefield state
2. A prediction model (codename _p10n_): Given a battlefield state, predict the action to perform (e.g. the action BattleAI would choose)

Combining 1. and 2. allows us to "imagine" one timestep like so:
Imagine it's our turn in the battle (say we are the defender). We have the observation (the battlefield state) and want to see what _would happen_ if we were to make action X. Instead of actually making the action, we want to simulate it using our model:
1. The state + action are passed to the `t10n` model, which produces a next_state. Let's assume in this next_state, it is the opponent's turn.
2. The next_state is passed to the `p10n` model, which produces a next_action to be taken by the opponent. I.e. it tries to guess what BattleAI would do in this situation.
3. Go back to step 1., passing next_state + next_action as inputs.

This loop continues until step 1. produces a state where it is our turn to act -- i.e. one full "timestep" is simulated. Being able to imagine timesteps, imagining trajectories is simply a matter of repeating this process (up to H times, where H stands for "horizon" - a configurable value).

// TODO: sequence diagram with vcmi part crosed-out, and replaced by an imagination model

Training t10n and p10n models was essentially a supervised learning problem where:
1. t10n is trained with (state, action) inputs and loss function L(predicted_next_state, real_next_state)
2. p10n is trained with (state) as input and loss function L(predicted_action, real_action)

This type of training does not require online sampling from the environment, i.e. I could collect (_lots_ of) samples separately, store them in S3 and later use them for training. Given that a sample consists of two observations and an action, that meant the size of one sample is ~100KB, which is _a lot_. I needed millions of samples, which  meant I needed terabytes of storage (100GB for 1M samples)! This leads to two big issues:
1. So much storage does not come cheap, as apart from the S3 costs, the VMs themselves would become much more costly since VastAI's instance pricing (naturally) comes with a per-GB of storage cost term.
2. Such huge volumes of data would require a lot of time to download, even with fast download link speeds.

Luckily, numpy has a builtin function for storing ndarrays as compressed data, so I used that to reduce the size of the collected samples by a factor of 10 to 20 (impressive!). Unfortunately, that also comes at a cost - the CPU processing needed to compress and uncompress those archived samples, but it's relatively a small price to pay. By using a combination of vector environments, pytorch data loaders and custom functions, I managed to collect millions of such samples.

I could now start training my t10n and p10n models. Firstly, I had to find a suitable loss function.

In the case of p10n, this was straightforward, since p10n outputs an action distribution which must be compared to a ground-truth one-hot encoding of the true action. For t10n, however, it was more complicated as the predicted/target state being compared encompass a variety of different features, such as creature attributes, damage dealt, etc. Moreover, those features also have different encodings:
1. Continuous features, e.g. "hit points": `10` => encoded as `0.01` (assuming max HP is 1000)
2. Binary features, e.g. "traits": [can_fly, can_shoot, double_strike, ...] encoded as `[0, 1, 0, ...]`
3. Categorical features, e.g. "army slot": `3` encoded as `[0,0,0,1,0,0,0], (assuming max slots is 7)

Using a simple MSE loss is really inefficient in this case, so I opted for separate losses per encoding type:
L = MSE(cont) + BCE(bin) + CE(cat)

where MSE is "mean squared error", BCE is "binary cross entropy" and CE is "cross entropy".

After a couple of months, I had trained a pair of decent p10n and t10n models which looked promising.
// todo: screenshots of W&B t10n, p10n charts

I was ready to put them to the test - however I quickly realized that the state produced by the t10n model requires some post-processing in order to be rendered. It is because its output is not discrete, i.e. binary values are not `0` or `1`, but something in between, e.g. `0.93`. That's because there's inherent uncertainty in some transitions: for example, when the action is "attack" with a Scorpicore, there's _a chance_ the victim gets paralyzed. The model accounts for that uncertainty and outputs a value for the "is_paralyzed" flag which is neither 0 nor 1, but something in between. I like the term _superposition_ here, as in order to turn such a result into a _real_ state which can be rendered, that state must be _collapsed_, e.g. make binaries exactly 0 or 1.

// todo: screenshots of predicted t10n states

It was now time to combine those into an environment model for simulating future timesteps :) I called it the "world" model:
// todo: screenshot of simulated timesteps

Due to the autoregressive nature of the process, there is a cumulative error at each step which tends to makes the imagined next observation rather "blurry". Continuing the example from above, if the Scorpicore's victim acts next (as part of the same imagined timestep) and the predicted action is to move it somewhere, the uncertainty spreads further as it might have been paralyzed, which is an invalid state per se (paralyzed creatures can't move). Add several layers of uncertainty on top of this, and you end up with a _mixture of possible futures_ which sometimes makes very little sense and is hard to reconstruct as invalid states may occur: the scorpicore's victim is paralyzed but at the same time it has moved somewhere, often resulting in _multiple ephemeral copies_ of a single unit in several locations: e.g. one is being the paralyzed unit which did not move, the other - the non-paralyzed unit which did move somewhere. To deal with this, I had to collapse each imagined transition to a discrete state then feed the collapsed state for the next forward passes (uncertainty after only one transition was manageable, so it could be collapsed easily).

The issue with reconstructing the imagined observation was now eliminated, and now I could clearly see my model hallucinating: some timesteps (those involving many enemy actions) caused too much distortion, and units often got transformed from one type to another. For example, First Aid tents (75 HP, 0 speed) sometimes ended up transformed into _something_ that looks like a Dendroid Guard (70 HP, 3 speed), but with mixed stats. Things get especially messy if the last unit of the army _might have been killed_, so the model believes the battle _might have ended_, and it becomes increasingly uncertain of _everything_ after that step:


In general, that model was "dreaming" decently, unfortunately MCTS algorithms require a much more stable environment model (ideally a true simulator of future steps) so the odds of this model working for those algos were really slim. Such algos were likely not worth the time to invest, given there were no good open-source implementations of those algos out there. I did give muax a try, given it was officially mentioned in google-deepmind's mctx repo, but it unfortunately turned out to be a shiny packaging for a rather naive implementation (no support for vectorized/batched observations, hardcoded for CPU, assumptions for float32 actions and observations, etc.) so it was not really useful. For good or bad, I found about those limitations too late in the process, but at least I learned to work with jax in the process. I had rewritten all my models to jax and flax, then had to rewrite them again for haiku (an "older" alternative to flax), but finally getting my hands dirty with jax was a new and exciting experience for me. Honestly, I like it _a lot_. It reminded me of the time I migrated from pure OOP to a pure functional programming language :) But let's not get carried away. MCTS algos were not looking so promising anymore and I already had a better idea: I2A.

After some more research, I stumbled upon a paper about I2A - Imagination Augmented Agents for deep reinforcement learning) where agents contain an "imagination core" module, responsible for simulating (imagining) trajectories in the RL environment without actually performing any steps in the environment itself. Most importantly, the authors claimed I2A works well with imperfect environment models. I wanted to give it a shot.
// todo: screenshot from I2A paper

My world model (combining t10n + p10n) seemed suitable for the task, so I got my hands dirty with the implementation of the I2A algo (will bore you with the details of that part). The end result, unfortunately, was disappointing: the _enormous_ computational overhead introduced by my "world" model combined with the huge action space (2312) destroyed all hopes for training an I2A model. Even with a modest I2A configuration - horizon=5 timesteps, trajectories=10 (less than 30% of the actions a unit can take on average), batch_size 10 steps) the model requires makes nearly 500 (yes, five hundred, including a third "reward" model which I didn't even mention) forward passes. This was a huge performance hit and made the learning process too slow. It was a disaster. I2A was not going to work for me. It was a rough rock to swallow and I almost gave up. But I had to move on.

That's when I went back to MPPO-DNA and started thinking about how can I decompose the huge action space. These 2312 discrete actions were causing trouble with basically all RL algos. I had previously attempted (unsuccessfuly) a strategy where the model has separate processing paths for the different input projections (spatial and non-spatial, inspired by the "An Introduction of mini-AlphaStar" paper) and where the final action is combines the outputs of 3 NN heads. This did seem like the proper way to go, though, so I decided to try it again, starting over with the implementation (trying to "fix" the existing implementation would lock me in the old patterns and likely produce the same result).
I started with an NN architecture improvement by implementing a custom 2D Convolutional NN layer since the conventional one is designed for a square grid, not a hex grid:
// todo: pictures of 2d convolution on square grid and hex grid

Another change in the architecture I made in the process was to incorporate self-attention (hexes<->hexes) via a Transformer layer as well as cross-attention (global state -> hexes):
// todo: diagram of the NN architecture with HexConv and Transformer

I played quite a bit with the hyperparameters of these networks, as I was sure that would eventually pay off - transformers, after all, have gotten so much _attention_ in the recent years, I had to give them a chance. And they did pay off... to a certain extent. For the first time, my models achieved 50% winrate vs. VCMI's BattleAI. This was definitely _something_, as I had had been unable to achieve MMAI supremacy so far:
// todo: screenshot of ntbovhkm run and train/ep_success_rate

However, my happiness was short-lived. The transformer-based models trained fast and well... until they didn't. As can be seen from the chart, the model experienced a sudden collapse shortly after reaching the 50% winrate. I can't explain why, but this happened to the other transformer-based models as well. Naturally, I tried to mitigate the issue by reducing the learning rate, introducing a max normalized gradient cap, lowering the number of transformer layers - all without success. Still, this experiment gave me some food for thought: propagating context between hexes without enforcing locality bounds (as is the case with convolutional layers) and without losing the spatial structure of the input (as is the case with MLP projections) was the proper way to go. That's why I decided dive deeper in a new type of neural network: the Graph Neural Network (GNN).

// todo: maybe an image of the graph neural network

GNNs are a relatively recent discovery: traditional NNs date back to ... (TODO), while GNNs emerged in .... (TODO). They are excellent for modeling objects (called graph nodes), their relationships (called graph edges) and the information processing flowing through them (called message passing). In fact, nearly _everything_ can be represented as graphs. MLPs, CNNs and Transformers can be considered a special case of graph neural networks. VCMI's battlefield can be efficiently represented via graphs as well, and that was all I needed to start working on a GNN-based model.

First of all, to represent the VCMI battlefield _as a graph_ meant it was time for some C++ coding and a new MMAI version: 13 (yes, I have 12 other versions, each with different representation of the VCMI battlefield state, but they've been omitted here to avoid bloating the article):

// todo: v13 observation PUML diagram + VCMI screenshot with graph elements on top for illustration purpose

Expressing observations via graphs feels _natural_, maybe it's because it gets really close to my cognitive perception of the game when playing it myself.
To give you an example, all this time I've been thinking how to encode information like "unit A can shoot at unit B" and it just wasn't easy without graphs -- add a fixed-size block of 165*10 "can_shoot" flags to the observation, effectively hard-coding an assumption of maximum 10 shooters on the battlefield as well as increasing observation size by 165*10 = 1650 floats = 6.6 KB. The reason this is super inefficient is the fact there are often less than 4 shooters on the battlefield, meaning 60% of those flags (or 4 KB of this observation) would always zeros for those battles, while the enforced limit of 10 shooters is naive -- if the player's army has 2 shooters and a ballista, when attacking a pack of neutrals which spread on 7 stacks, there's already more shooters than can the observation can handle. This is an example of just _one_ of the many relations -- graphs are a clear winner for encoding such information.
With the MMAI v13 observation structured as a graph, I could now leverage the power of GNNs. There were many 

The initial results, however, did not meet my expectations: these models turned out mediocre at best, overall worse than my conventional NN-based models. In addition, the training was very slow as [GNN batching](https://pytorch-geometric.readthedocs.io/en/2.4.0/notes/batching.html) is different than the conventional batching and I had deferred the GNN batching for a later stage of development. In general, it looked bad and landed a heavy blow on my motivation, as I had high hopes for those models. That's when I felt I should take a break from the project. A couple of months later, I was back and ready to try again, throwing away my previous implementation and writing an all-new GNN-based MPPO training script. This time I properly batched my inputs, plus refined the NN architecture (the diagram shown above actually reflects this new architecture). I also fanned out half a dozen training experiments in parallel, trying out different GNN types:
* [GCNConv](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.nn.conv.GCNConv.html), the "classic" Graph Convolutional network
* [GATv2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html)
* [TransformerConv](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.nn.conv.TransformerConv.html) (Graph Attention v2 convolutional network)
* [GINEConv](https://pytorch-geometric.readthedocs.io/en/2.5.1/generated/torch_geometric.nn.conv.GINEConv.html) (modified Graph Isomorphism network)
* [PNAConv](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.nn.conv.PNAConv.html) ( Principal Neighbourhood Aggregation graph convolutional network)
* [GENConv](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.nn.conv.GENConv.html) (Generalized Graph Convolution network)

In addition, I also tried various graph designs, e.g. static vs. dynamic graph nodes, undirected vs uni-directional vs bi-directional graph edges, various message passing functions, GNN layers, homogenous vs heterogenous graphs, attention-based scoring, etc. To make those experiments feasible, I had to overcome the issue related to my lack of proper hardware to train on. Using home laptops was simply not gonna cut it: the feedback cycle was too slow. Each change I implemented took days or even weeks of training until I could see the results from it. Such hardware also imposed a bottleneck the number of concurrent experiments that could be carried out, essentially enforcing sequential order. I needed _better_ hardware. That's when I began considering options for buyng or renting hardware. Both seemed too expensive for me, but then I remembered a guy I met at an AI hackathon event some time ago who told me about Vast AI where I could (relatively) cheaply rent consumer GPU hardware. I did a quick research and it did turn out that VastAI is a much better alternative to the datacenter/enterprise rentals out there. Plus, you can also rent some pretty good gamer-grade CPUs (which is exactly what I was looking for, since my samples were being collected on-the-fly as the game was running at very high speeds) - top-notch gaming machines such an RTX5090 GPU with a Ryzen 9900X CPU for under $0.20/hr (at least those were the prices as of 2025).
In addition, my free-tier storage on VastAI was running low and was also not a feasible approach anymore, so I migrated to AWS S3.
With all these improvements, I now faced another bottleneck -- the GPU was under-utilized! Running 30 PBT agents in parallel, each running a single instance of VCMI, collecting observations and performing gradient calculations independently, was not efficient. Such GPUs work best when the operations are performed in large batches. So I ditched the PBT approach (honestly, PBT had done its job - I now had a pretty good idea which hyperparameter configuration which worked best for the task). Instead, I opted for gymnasium's vector environments which are perfect for the task, although I had to patch the VectorEnv in order to make it work with VCMI. With all those improvements in place, I managed to achieve a significant speed-up in the training process.

And although I managed to cover only a tiny fraction of the possible configurations with those experiments, some looked quite promising, most notably those involving GENConv networks. I started experimenting further and found out that a dynamic node heterogenous graph with directed edges and attention-based scoring works pretty well. Below is a diagram of the final architecture used:

// todo: mppo_dna_gnn NN architecture diagram
// todo: w&b charts of best runs

At this point, MMAI played **better** than VCMI's strongest script-based AI. I watched the winrate climb to nearly 70% and finally felt the relief. It was a big milestone and a worthy successor to the older models, it was time to give it a playtest. However, there was a new problem: C++ compatibility. GNNs worked fine in a python environment with pytorch-geometric, but in order to work in VCMI they had to be exported to a compatible format for loading in C++, which is not straight-forward as libtorch can't load GNN models. Moreover, the approach I had used before (with the conventional NNs) involved using TorchScript for model exports, but that meant relying on the pytorch mobile module which is deprecated in favor of [executorch](https://docs.pytorch.org/executorch/stable/index.html). So the next challenge was to figure out how to export the model with executorch, effectively _lowering_ it to a set of simple operations loadable by executorch.

Executorch provides an API to various [backends](https://docs.pytorch.org/executorch/stable/backends-section.html#choosing-a-backend), e.g. Core ML on iOS, Arm CPU or Vulkan on Android, etc. This meant I had to export a model for a specific backend, then use that same backend to load the model in VCMI. The trouble was that VCMI aims to support nearly all platforms you can think of (linux, windows, mac, android, ios, many of them with options for x86, x64 or arm architectures) and most of the backends mentioned earlier only work on specific platforms. For example, a CoreML model can't be used on anything other than iOS and MacOS. Same goes for Vulkan models, except it's for Android. Many backends target a specific manufacturer, such as Qualcomm or MediaTek SoCs. The _only_ executorch backend which was supported on all those platforms was XNNPACK, so I went that road and tried to do an XNN export, which turned out less straight-forward than I thought, as there were many operations that could not be lowered directly to XNNPACK instructions. Stuff like dynamic tensor shapes,  smart indexing, operations like softmax, argmax and sampling had to be reworked using only low-level calls such as `scatter`, `gather`, `index_select`, etc.. Abstractions from pytorch-geometric such as `HeteroData` and `HeteroBatch` had to be broken down to simple tensors with _fixed_ shapes, float `inf`s had to be replaced with huge negatives and so on. Redesigning the model to work with fixed size input tensors was was achieved using fixed-shape masks for all operations. Since the input graph is "dynamic", the input tensors varied in size depending the battlefield and that difference sometimes reached orders of magnitude, which meant I had to pre-define several different fixed sizes for the input tensors and to accomodate for the graph dynamism without wasting memory and compute. I pre-defined several "buckets" which correspond to different battlefield sizes, e.g. small buckets for battlefields with only a few slow melee stacks. The bucket sizes were defined using statistical information collected over 10000 different observations:
// TODO: table with size statistics

It took me quite some time to make all those changes and ChatGPT's help was invaluable for this part, as some of the lower-level tensor operations can really make your head spin. On top of that, the implementation details in pytorch-geometric are far from pretty, there are some abstractions in there which turn a simple message passing process into deeply nested calls, plus as there's a lot of dead code (apparently they did support model lowering at some point in the past which was fully deprecated by 2.5.0, but the relevant code was still polluting the codebase). I won't bother you with further details. In the end it was a useful experience for me, as it helped me understand the inner workings of the GNN implementation (specifically GENConv networks). So now I had an XNNPACK model which I could load in VCMI using the executorch C++ API. And it worked! I played several VCMI battles vs. my model and confirmed that it had indeed learned some peculiar mechanics, such as keeping outside of enemy movement or shooting range, blocking shooters, leaving blind enemies alone, etc. Unfortunately, it was a bit slow - on a Mac M1, a single action prediction on a relatively small battlefield took 160ms on average which was not ideal. Moreover, on a mid-range Android smartphone from 2022 this time jumped to the whopping 700ms per prediction - imagine playing a single-player game with 6 computer players, each of which fights one or more battles on their turn... it would take _minutes_ until all computers end their turn. On a budget iPhone from 2020, the performance was much better - 160ms per prediction - but still bad overall:
// TODO: table with executorch benchmark

In addition, C++ executorch failed to compile for x86 architectures. For windows, it compiled successfully for x64 arch, but a nasty UB occurred as there were apparently some issues with the memory management in executorch's XNNPACK implementation, leading to weird predictions outside the action space range, sometimes crashing the entire application due to memory violation. I suspect that the issue stemmed from `long` data type is 4 bytes instead of 8, but I was unable to confirm that. Not to mention 32-bit architectures were out of the question, although VCMI does has 32-bit windows, linux and android builds. I had to find another solution, so I turned my attention at other executorch backends in an attempt to improve the performance during inference, despite knowing that also means distributing several different model exports and using conditional logic for selecting the appropriate export at runtime based on the current platform. Each of them meant additional modifications to the model as each framework supports different opsets. There's a lot of boring details here but I will try to keep it short. Vulkan was not stable and required workarounds to make the export work, the c++ code failed to compile on linux (had to use a macos runner) and ultimately failed to load models due to a missing sum_int32 shader error. CoreML produced 3x larger models (over 100MB in size) which generally performed well on apple devices, if it weren't for their "cold" start, which on iOS caused a 10-second (!) freeze on the first forward pass. After all this, I resorted to libtorch again. Since libtorch took up to 5 hours (!!!) to compile from source, I had to (compile it separately)[https://github.com/smanolloff/vcmi-libtorch-builds] and download it as a compiled artifact to link against during the VCMI build. However, the exported models had a smaller footprint, better performance and wider OS support. 32-bit CPU architectures were still out of the question, but overall it was OK.

After wiring it all up, I continued to playtest the models and discovered that, although it played well, it was still too defensive: waiting for enemy units to move within range, before it would attack. I had to find a way to suppress this behaviour as it was outlined as one of the issues in the previous versions. It took a few tweaks to the reward function and (after training a new model from scratch) the behaviour was _rectified_ at the expense of a slight drop in the winrate vs. BattleAI.
// TODO: w&b diagrams with ep_len_mean for models with different step_reward

And so, in October 2025, I submitted my new, smarter and behaviour-corrected MMAI models to VCMI (actually, updated the old PR which was still open after exactly 1 year). During the usual PR stuff (code reviews, comments, etc.), _Laserlicht_ (one of the VCMI devs) suggested I should replace `libtorch` with [onnxruntime](https://onnxruntime.ai/), as there was a `conan` recipe for it and seemed to offer wider OS support. I decided to give it a try, so I tweaked the model a bit to be able to export it as an ONNX file, wrote the corresponding model loader in VCMI and gave it a spin. The results were surprising - the model was predicting _faster_ by nearly 30% compared to libtorch! The integration also felt overall smoother, and the OS/hardware support was better. With some help from _GeorgeK1ng_ (another VCMI dev), we even managed successfully compile  32-bit Windows artifacts. All platforms targeted by VCMI were now able to run MMAI models through `onnxruntime` and that was simply awesome! The PR is ready for merge now and MMAI will likely be included in the next major VCMI release v1.7 :)


